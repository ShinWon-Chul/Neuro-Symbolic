{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Theorem Prover using pandas and Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Symbolic Unificaiton using pandas DataFrame\n",
    "- Load Files(Config, KG, Rule Template, ...)\n",
    "- Generate Meta Tables(Rule Structure, KG index, ...)\n",
    "- Run Backward Chaining and generate batch \n",
    "\n",
    "## 2. NTP Model Training with PyTorch\n",
    "- Define Model Structure using PyTorch\n",
    "- Define Foward Function \n",
    "- Training Model\n",
    "\n",
    "## 3. Extract Rules from Trained Embedding Vectors\n",
    "- Matching Rule templates with Embedding vectors \n",
    "- Extract Induced Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom functions\n",
    "from util.fileUtils import load_conf, load_from_file, create_directory\n",
    "from ntp.prover import backward_chaining\n",
    "from preprocess.dataPreprocessing import data_filter, padding\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from pprint import pprint\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# to print pandas dataframe\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_name options = example_7, kinship, umls, nations\n",
    "data_name = 'example_7'\n",
    "\n",
    "random.seed(1337)\n",
    "torch.manual_seed(1337)\n",
    "torch.cuda.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size :  5\n"
     ]
    }
   ],
   "source": [
    "config = load_conf(f\"./config/{data_name}.conf\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_size = config['model']['embedding_size']\n",
    "drop_prob = config['model']['drop_prob']\n",
    "weight_decay = config['model']['l2']\n",
    "epochs = config['training']['num_epochs']\n",
    "report_interver_epoch = config['training']['report_interval']\n",
    "learning_rate = config['training']['learning_rate']\n",
    "neg_per_pos = config['training']['neg_per_pos']\n",
    "batch_size = config['training']['batch_size']\n",
    "init = config['model']['init']\n",
    "result_dir = config['meta']['result_dir']\n",
    "result_file = config['meta']['result_file']\n",
    "shuffle = config['training']['shuffle']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data Files using pandas\n",
    "- KG : Knowledge Graph file with triple form\n",
    "- Query : query with triple form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>subj</th>\n",
       "      <th>obj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nationality</td>\n",
       "      <td>BART</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>placeOfBirth</td>\n",
       "      <td>BART</td>\n",
       "      <td>NEWYORK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>locatedIn</td>\n",
       "      <td>NEWYORK</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hasFather</td>\n",
       "      <td>BART</td>\n",
       "      <td>HOMMER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nationality</td>\n",
       "      <td>HOMMER</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pred     subj      obj\n",
       "0   nationality     BART      USA\n",
       "1  placeOfBirth     BART  NEWYORK\n",
       "2     locatedIn  NEWYORK      USA\n",
       "3     hasFather     BART   HOMMER\n",
       "4   nationality   HOMMER      USA"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KG = KG[['pred', 'subj', 'obj']]\n",
    "KG.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>subj</th>\n",
       "      <th>obj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>placeOfBirth</td>\n",
       "      <td>HOMMER</td>\n",
       "      <td>NEWYORK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hasFather</td>\n",
       "      <td>BART</td>\n",
       "      <td>HOMMER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bornIn</td>\n",
       "      <td>BART</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nationality</td>\n",
       "      <td>BART</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>placeOfBirth</td>\n",
       "      <td>BART</td>\n",
       "      <td>NEWYORK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pred    subj      obj\n",
       "0  placeOfBirth  HOMMER  NEWYORK\n",
       "1     hasFather    BART   HOMMER\n",
       "2        bornIn    BART      USA\n",
       "3   nationality    BART      USA\n",
       "4  placeOfBirth    BART  NEWYORK"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Query = Query.sample(frac=1).reset_index(drop=True)\n",
    "Query = Query[['pred', 'subj', 'obj']]\n",
    "Query.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_list = sorted(set(KG.subj.values).union(set(KG.obj.values)))\n",
    "len(entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting time :  0:00:00.000999\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "#KG index dictionary initializing\n",
    "KG_index = {}\n",
    "for entity in entity_list:\n",
    "    KG_index[entity] = {'subj':[], 'obj':[]}\n",
    "    \n",
    "subj_entities = KG['subj'].tolist()\n",
    "obj_entities = KG['obj'].tolist()\n",
    "\n",
    "#KG index dictionary generation\n",
    "for i in range(len(KG)):\n",
    "    KG_index[subj_entities[i]]['subj'] = KG_index.get(subj_entities[i]).get('subj')+[i]\n",
    "    KG_index[obj_entities[i]]['obj'] = KG_index.get(obj_entities[i]).get('obj')+[i]\n",
    "\n",
    "end = datetime.now() \n",
    "print('converting time : ', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BART': {'subj': [0, 1, 3, 6], 'obj': []},\n",
       " 'HOMMER': {'subj': [4, 5], 'obj': [3]},\n",
       " 'NEWYORK': {'subj': [2], 'obj': [1, 5]},\n",
       " 'USA': {'subj': [], 'obj': [0, 2, 4, 6]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KG_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Rule template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules, max_atom = load_from_file(config['data']['templates'])\n",
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('#1', 'X', 'Y'), ('#2', 'X', 'Z'), ('#3', 'Z', 'Y'), 2],\n",
       " [('#1', 'X', 'Y'), ('#2', 'X', 'Y'), 2]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule_structure = pd.DataFrame(list(map(lambda x : [{atom[1]: 'subj', atom[2]: 'obj'} for atom in x[:-1]], rules)))\n",
    "rule_structure['rule_number'] = [i for i in range(len(rules))]\n",
    "rule_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dictionary from KG & Query data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "KG_predicate_list = sorted(set(KG.pred.values).union(set(Query.pred.values)))\n",
    "\n",
    "rule_pred_list = []\n",
    "for i, rule in enumerate(rules):\n",
    "    # iterate rule components\n",
    "    for r in rule[:-1]:\n",
    "        #iterate augmnet number\n",
    "        for j in range(rule[-1]):\n",
    "            suffix = '_' + str(i) + '_' + str(j)\n",
    "            rule_pred_list.append(r[0]+suffix)\n",
    "            \n",
    "predicate_list = sorted(set(KG_predicate_list).union(set(rule_pred_list)))\n",
    "# print('predicates : ',predicate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2sym_dict = {}\n",
    "sym2id_dict = {}\n",
    "sym2id_dict['UNK'] = 0\n",
    "sym2id_dict['PAD'] = 1\n",
    "id2sym_dict[0] = 'UNK'\n",
    "id2sym_dict[1] = 'PAD'\n",
    "\n",
    "\n",
    "for i, p in enumerate(predicate_list):\n",
    "    sym2id_dict[p] = i+2\n",
    "    id2sym_dict[i+2] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 0,\n",
       " 'PAD': 1,\n",
       " '#1_0_0': 2,\n",
       " '#1_0_1': 3,\n",
       " '#1_1_0': 4,\n",
       " '#1_1_1': 5,\n",
       " '#2_0_0': 6,\n",
       " '#2_0_1': 7,\n",
       " '#2_1_0': 8,\n",
       " '#2_1_1': 9,\n",
       " '#3_0_0': 10,\n",
       " '#3_0_1': 11,\n",
       " 'bornIn': 12,\n",
       " 'hasFather': 13,\n",
       " 'locatedIn': 14,\n",
       " 'nationality': 15,\n",
       " 'placeOfBirth': 16}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sym2id_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Backward Chaining\n",
    "\n",
    "#### unification\n",
    "- goal: query (e.g. nationality BART USA)\n",
    "- rule: rule template (e.g. #1(X,Y) :- #2(X,Z), #3(Z,Y))\n",
    "\n",
    "- 1. 주어진 rule template의 conclusion과 query를 unify\n",
    "    - unify된 트리플은 rule component substitution에 key를 rule component(e.g. #1(X,Y))로  \n",
    "        value를 unified triples(dataframe)으로 저장   \n",
    "    \n",
    "        #1(X, Y) :\n",
    "    \n",
    "            |     pred    | subj | obj |\n",
    "            |-------------|------|-----|\n",
    "            | nationality | BART | USA |\n",
    "    \n",
    "    - conclusion의 X,Y와 같은 variable에 대하여 unify된 트리플을 참조하여 variable substitution에  \n",
    "    X : [BART], Y: [USA] 와 같이 binding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- 2. 앞서 binding된 variable을 참조하여 각 rule body에 맞는 트리플을 unify\n",
    "    - #1(X,Y)를 통해 binding된 X에 대한 variable substitution을 참조하여 #2(X,Z)와 같은 body에 트리플을 unify하는 작업을 수행\n",
    "        - 위 경우에는 variable substitution을 참조하여 X가 subject인 트리플을 찾아 unify   \n",
    "    - unify된 트리플은 rule component substitution에 key를 rule component(e.g. #2(X,Y))로  \n",
    "        value를 unified triples(dataframe)으로 저장   \n",
    "           \n",
    "       #2(X, Z) :\n",
    "\n",
    "            |     pred     | subj | obj     |\n",
    "            |--------------|------|---------|\n",
    "            | placeOfBirth | BART | NEWYORK |\n",
    "            | hasFather    | BART | HOMMER  |    \n",
    "        \n",
    "    - 규칙 body의 X,Z와 같은 variable에 대하여 unify된 트리플을 참조하여 variable substitution에  \n",
    "    Z : [NEWYORK, HOMMER] 와 같이 binding\n",
    "    \n",
    "#### proof path completion\n",
    "- rule component substitution : key가 rule compnent (e.g. #1(X,Y)) value가 각 rule component에 unify된 트리플(dataframe)인 dictionary \n",
    "- rule: rule template (e.g. #1(X,Y) :- #2(X,Z), #3(Z,Y))\n",
    "- 1. rule template을 분석하여 인접한 rule component간의 common variable 도출 \n",
    "- 2. common variable을 기준으로 unified triple을 join하여 proof path를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_path, rule_temp_path, max_path = backward_chaining(Query, KG, KG_index, \n",
    "                                                            rules, rule_structure, sym2id_dict, neg_per_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data filtering\n",
    "    - proof path가 없는 데이터 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_path = list(filter(data_filter, relation_path))\n",
    "rule_temp_path = list(filter(data_filter, rule_temp_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_path, rule_temp_path = padding(relation_path, rule_temp_path, rules, max_path, \n",
    "                                        max_atom, neg_per_pos, sym2id_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_tensor(path_data):\n",
    "    path_tensor = []\n",
    "    for i in path_data:\n",
    "        for j in i:\n",
    "            path_tensor.append(j)\n",
    "    path_tensor = torch.tensor(path_tensor)\n",
    "    return path_tensor\n",
    "\n",
    "relation_tensor = convert_list_to_tensor(relation_path)\n",
    "rule_temp_tensor = convert_list_to_tensor(rule_temp_path)\n",
    "\n",
    "answer = [1]\n",
    "for j in range(neg_per_pos):\n",
    "    answer += [0]\n",
    "answer = torch.tensor(answer, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class proof_path_dataset(Dataset): \n",
    "    def __init__(self, relation_tensor, rule_temp_tensor, label):\n",
    "        self.relation_tensor = relation_tensor\n",
    "        self.rule_temp_tensor = rule_temp_tensor\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.relation_tensor)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        rel_path = self.relation_tensor[idx]\n",
    "        rule_tamplate= self.rule_temp_tensor[idx]\n",
    "        label = self.label\n",
    "        return rel_path, rule_tamplate, label\n",
    "    \n",
    "\n",
    "dataset = proof_path_dataset(relation_tensor, rule_temp_tensor, answer)\n",
    "batch_generator = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTP(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size, max_path, batch_size,  num_templates, dropout, device):\n",
    "        super(NTP, self).__init__()\n",
    "        self.device = device\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding_matrix = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.max_path = max_path\n",
    "        self.batch_size = batch_size\n",
    "        self.template_size = num_templates\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def l2_sim(self, embed_aug_rule_temp_path, embed_aug_rel_path):\n",
    "        eps = 1e-6\n",
    "        dist = torch.sqrt((embed_aug_rel_path - embed_aug_rule_temp_path).pow(2).sum(4)+eps)\n",
    "        sim = torch.exp(-dist)\n",
    "        return sim\n",
    "    \n",
    "    def calculate_sim_avg(self, aug_rule_temp_path, aug_rel_path):\n",
    "\n",
    "        embed_aug_rule_temp_path = self.embedding_matrix(aug_rule_temp_path)\n",
    "        embed_aug_rel_path = self.embedding_matrix(aug_rel_path)\n",
    "                                                         \n",
    "        embed_aug_rule_temp_path = self.dropout(embed_aug_rule_temp_path)\n",
    "        embed_aug_rel_path = self.dropout(embed_aug_rel_path)\n",
    "        \n",
    "        sims=self.l2_sim(embed_aug_rule_temp_path, embed_aug_rel_path)\n",
    "        avg_sims = torch.mean(sims, 3)\n",
    "\n",
    "        return avg_sims\n",
    "        \n",
    "        \n",
    "    def forward(self, aug_rule_temp_path, aug_rel_path):\n",
    "        \n",
    "        avg_sims = self.calculate_sim_avg(aug_rule_temp_path, aug_rel_path)\n",
    "        max_sims = torch.max(avg_sims, axis=2)[0]\n",
    "        min_sims = torch.min(max_sims.view(-1, self.max_path), axis=1)[0]\n",
    "        \n",
    "        return min_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NTP(\n",
       "  (embedding_matrix): Embedding(17, 100)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_templates = len(rules)\n",
    "vocab_size = len(sym2id_dict)\n",
    "ntp = NTP(vocab_size, embedding_size, max_path, batch_size, num_templates, drop_prob, device)\n",
    "ntp.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "if init:\n",
    "    ntp.apply(initialize_weights); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Relation Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10      Loss : 0.474\n",
      "Epoch: 20      Loss : 0.448\n",
      "Epoch: 30      Loss : 0.421\n",
      "Epoch: 40      Loss : 0.397\n",
      "Epoch: 50      Loss : 0.378\n",
      "Epoch: 60      Loss : 0.365\n",
      "Epoch: 70      Loss : 0.353\n",
      "Epoch: 80      Loss : 0.353\n",
      "Epoch: 90      Loss : 0.347\n",
      "Epoch: 100      Loss : 0.343\n",
      "Epoch: 110      Loss : 0.325\n",
      "Epoch: 120      Loss : 0.327\n",
      "Epoch: 130      Loss : 0.297\n",
      "Epoch: 140      Loss : 0.325\n",
      "Epoch: 150      Loss : 0.318\n",
      "Epoch: 160      Loss : 0.300\n",
      "Epoch: 170      Loss : 0.290\n",
      "Epoch: 180      Loss : 0.284\n",
      "Epoch: 190      Loss : 0.284\n",
      "Epoch: 200      Loss : 0.302\n",
      "\n",
      "training time :  0:00:06.346580\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(ntp.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "ntp.train()\n",
    "epoch_loss = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    for aug_rel_path, aug_rule_temp_path, label in batch_generator:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        label = torch.flatten(label).to(device)\n",
    "        \n",
    "        y_hat = ntp.forward(aug_rule_temp_path.to(device), aug_rel_path.to(device))\n",
    "        \n",
    "        loss = criterion(y_hat, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    if epoch%report_interver_epoch == 0:\n",
    "        end_time = datetime.now()\n",
    "        print(f'Epoch Time: {end_time-start_time} \\tEpoch: {epoch}\\tLoss: {epoch_loss/epoch:.3f}')\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('\\nTotal Training Time : ', end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write rule file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representation_match(x, emb):\n",
    "    dist = torch.torch.nn.functional.pairwise_distance(x, emb)\n",
    "    sim = torch.exp(-dist)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-8.7272e-09,  1.6659e-11,  5.6860e-14,  ..., -1.1509e-14,\n",
      "          1.5266e-21,  2.5696e-18],\n",
      "        [ 3.6398e-19, -9.8390e-20,  4.5670e-20,  ..., -1.0445e-17,\n",
      "         -1.2637e-18, -1.2327e-17],\n",
      "        [ 1.6603e-02,  1.0894e-01, -1.0419e-01,  ...,  9.1910e-02,\n",
      "          5.6643e-02,  2.1096e-02],\n",
      "        ...,\n",
      "        [-4.3441e-01, -5.0007e-01,  5.1704e-01,  ..., -5.4621e-01,\n",
      "         -2.8323e-01,  4.7105e-01],\n",
      "        [-1.8260e-02,  1.2069e-01, -9.7959e-02,  ...,  9.0398e-02,\n",
      "          6.1292e-02,  3.6241e-02],\n",
      "        [ 1.1912e-01, -4.1318e-02,  3.7512e-02,  ..., -1.3295e-01,\n",
      "         -1.1405e-01, -1.6196e-01]], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#get trained embedding matrix\n",
    "for i in enumerate(ntp.parameters()):\n",
    "    print(i[1])\n",
    "    embeddings = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(('p0', 'X', 'Y'),\n",
       "  ('p1', 'X', 'Z'),\n",
       "  ('p2', 'Z', 'Y')): [[[2, 'X', 'Y'],\n",
       "   [6, 'X', 'Z'],\n",
       "   [10, 'Z', 'Y']], [[3, 'X', 'Y'], [7, 'X', 'Z'], [11, 'Z', 'Y']]],\n",
       " (('p0', 'X', 'Y'), ('p1', 'X', 'Y')): [[[4, 'X', 'Y'], [8, 'X', 'Y']],\n",
       "  [[5, 'X', 'Y'], [9, 'X', 'Y']]]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get parameterized rule template\n",
    "rule_templates = {}\n",
    "idx_rule_templates = {}\n",
    "for rule_number, template in enumerate(rules):\n",
    "    result_template_key = []\n",
    "    ids_result_template_value = []\n",
    "    ids_result_template_values = []\n",
    "    for i in range(len(template)-1):\n",
    "        rule_element=('p'+ str(int(template[i][0][1])-1), template[i][1], template[i][2])       \n",
    "        result_template_key.append(rule_element)\n",
    "        rule_element = ()\n",
    "\n",
    "    for aug in range(template[-1]):\n",
    "        for j in range(len(template)-1):\n",
    "            ids_result_template_value.append([sym2id_dict[template[j][0]+'_'+str(rule_number)+'_'+\n",
    "                                                           str(aug)], template[j][1], template[j][2]])\n",
    "        ids_result_template_values.append(ids_result_template_value)\n",
    "        ids_result_template_value = []\n",
    "    idx_rule_templates[tuple(result_template_key)] = ids_result_template_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[((('p0', 'X', 'Y'), ('p1', 'X', 'Z'), ('p2', 'Z', 'Y')),\n",
       "   0.23208558559417725,\n",
       "   ['nationality(X,Y)', 'hasFather(X,Z)', 'nationality(Z,Y)'])],\n",
       " [((('p0', 'X', 'Y'), ('p1', 'X', 'Z'), ('p2', 'Z', 'Y')),\n",
       "   0.1063181534409523,\n",
       "   ['nationality(X,Y)', 'placeOfBirth(X,Z)', 'locatedIn(Z,Y)'])],\n",
       " [((('p0', 'X', 'Y'), ('p1', 'X', 'Y')),\n",
       "   0.321990430355072,\n",
       "   ['bornIn(X,Y)', 'nationality(X,Y)'])],\n",
       " [((('p0', 'X', 'Y'), ('p1', 'X', 'Y')),\n",
       "   0.5807282328605652,\n",
       "   ['nationality(X,Y)', 'bornIn(X,Y)'])]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get rule instance & write rule file\n",
    "\n",
    "# 자기 자신을 masking하기 위한 rule template의 relation index생성\n",
    "masking_index = []\n",
    "for key, rules in idx_rule_templates.items():\n",
    "    for rule in rules:\n",
    "        for element in rule:\n",
    "            masking_index.append(element[0])\n",
    "            \n",
    "create_directory(result_dir)\n",
    "with open(result_dir + result_file, 'w') as f:\n",
    "    for key, rules in idx_rule_templates.items():\n",
    "        result = []\n",
    "        f.write(str(key)+'\\n')\n",
    "        for rule in rules:\n",
    "            relation_similarities = []\n",
    "            rule_result = []\n",
    "            for element in rule:\n",
    "                masking_index = masking_index+[element[0]]+[0, 1]\n",
    "                x = ntp.embedding_matrix(torch.tensor([element[0]]).to(device))\n",
    "                match = representation_match(x, embeddings)\n",
    "                match[masking_index] = 0\n",
    "                top_k = torch.topk(match, 1)\n",
    "                rule_result.append(id2sym_dict[top_k.indices.item()]+'('+element[1]+','+element[2]+')')\n",
    "                relation_similarities.append(match[top_k.indices])\n",
    "            confidence_score = str(min(relation_similarities).item())\n",
    "\n",
    "            head = rule_result[0]\n",
    "            body = rule_result[1:]\n",
    "            \n",
    "            result.append((round(min(relation_similarities).item(), 6), head + ' :- ' +\", \".join(body)+'\\n'))\n",
    "            result.sort(reverse = True)\n",
    "        for score, rule in result:\n",
    "            f.write(str(score)+ '\\t' + rule)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
