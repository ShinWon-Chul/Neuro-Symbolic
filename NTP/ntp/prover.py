import pandas as pd
import itertools
import collections
import copy
import random

def check_duplicate(proof_path):
    """
    - function : remove proof path containing duplicate triples

    param:
    proof_path(DataFrame) -- proof path for Qeury obtained through backward chaining
    
    return:
    proof_path(DataFrame) -- proof path with duplicate triples removed
    """
    triple_size = 3
    column_len = len(proof_path.columns)
    num_unique_triples = int(column_len/triple_size) 

    proof_path['n_unique_triples'] = \
        proof_path.apply(lambda row: len(set([tuple(row[ i*triple_size : (i+1)*triple_size ]) 
                                                    for i in range(num_unique_triples)])), axis=1)

    proof_path = proof_path[proof_path.n_unique_triples == num_unique_triples]
    proof_path
    
    return proof_path

def merge_from_common_key(current_rComp, next_rComp):
    """
    - function : 
        1. Finding common variables between two different rule components
        2. Create proof path by joining based on common variables

    param:
    current_rComp(DataFrame) -- 
        Triples unified to the rule component of the current depth (e.g. Triples corresponding to #1 (X, Y))
    next_rComp(DataFrame) -- 
        Triples unified to the next depth rule component (e.g. Triples corresponding to #2 (X, Z))

    In the above example, Dataframe has '#1', 'X', 'Y' columns and '#2', 'X', 'Z' columns, respectively, 
    and can be inner joined with a common variable 'X'.

    return: 
    proof_path -- Proof path created by joining with common variable (DataFrame) 
    """
    key = current_rComp.columns.intersection(next_rComp.columns, sort=False)
    new_key = key + '_'
    d = dict(zip(key, new_key))

    proof_path = pd.merge(current_rComp, next_rComp.rename(columns=d), how='inner', 
                          left_on=key.tolist(), right_on=new_key.tolist())

    return proof_path

def proof_path_completion(rComp_substitution, rule):
    """
    - function : 
        1. Create proof path by calling merge_from_common_key function
        2. Grouping triple relations and rule relations in the generated proof path

    param:
    rComp_substitution(dicrionary) -- a dictionary which has information of unified rule component
        - Key(tuple) : rule components (such as ('#1', 'X', 'Y'), ('#2', 'X', 'Z'), ('#3', 'Z', 'Y'))
        - Value(DataFrame) : triples unified in the rule component, Each DataFrame column can be configured as follows
                             Index(['#1', 'X', 'Y'], dtype='object')
                             Index(['#2', 'X', 'Z'], dtype='object')
                             Index(['#3', 'Z', 'Y'], dtype='object')
    rule(list) -- rule template (such as [('#1', 'X', 'Y'), ('#2', 'X', 'Z'), ('#3', 'Z', 'Y')])
    
    return
    unified_rel_path -- unified relation group (list of list) 
                        (such as [[(#1, nationalit), (#2, hasFather), (#3, nationality)], 
                                  [(#1, nationalit), (#2, birthplace), (#3, locatedIn)]])
    """  
    for depth in range(len(rule)-2):    
        if depth == 0:
            #Load dataframes
            current_rComp = rComp_substitution[rule[depth]] 
            next_rComp = rComp_substitution[rule[depth+1]]
            #merge from common key
            partial_proof_path = merge_from_common_key(current_rComp, next_rComp)

        else:
            #Load dataframes
            current_rComp = partial_proof_path
            next_rComp = rComp_substitution[rule[depth+1]]
            #merge from common key
            partial_proof_path = merge_from_common_key(current_rComp, next_rComp)

    #remove duplicate  Adjacent triple
    proof_path = partial_proof_path
    proof_path = check_duplicate(proof_path)
    
    #stackoverflow How to get that result without for loop (python)
    unified_rel_path = proof_path[[r[0] for r in rule[:-1]]].drop_duplicates()\
           .apply(lambda x: list(zip([r[0] for r in rule[:-1]], x)), axis=1).values.tolist()
    
    return unified_rel_path

def unification(goal, rule, KG, KG_index, rule_num, depth = 0, 
                variable_substitution={}, rComp_substitution={}, rule_structure=None):
    '''
    - function: 
        1. Unify Variables and store information in substitution dictionary
        2. Check Common Variable from Rules and Join each Triples
        3. return proof paths(relation group) generated by Symbolic Unification 
  
    param:
    goal(list) -- a query triple (e.g. [nationality BART USA])
    rule(list) -- a given rule template (e.g. [('#1', 'X', 'Y'), ('#2', 'X', 'Y'), 3])
    KG(DataFarme) -- Knowledge Graph
        -Index(['pred', 'subj', 'obj'], dtype='object')
    KG_index(dictionary) -- Hash table that maps DataFrame index according to KG's entity information
    rule_num(int) -- rule number
    depth(int) -- an integer indicates rule depth
    variable_substitution(dictionary) -- a dictionary which has information of unified variables
        - key: variable / value : unified entity (list)
    rComp_substitution(dictionary) -- a dictionary which has information of unified rule components
        - key: rule compoenet(tuple) / value : unified triples (dataframe)
    rule_structure(DataFrame) -- Meta table with all rule template component information and rule numbers
    

    return:
    unified_rel_path -- unified relation group (list of list) 
                        (such as [[(#1, nationalit), (#2, hasFather), (#3, nationality)], 
                                  [(#1, nationalit), (#2, birthplace), (#3, locatedIn)]])
    '''

    if depth == 0 :
        rComp_substitution[rule[depth]] = pd.DataFrame(goal, index=[rule[depth][0],rule[depth][1],rule[depth][2]]).transpose()
        # subject variable binding
        if rule[depth][1] not in variable_substitution.keys():
            variable_substitution[rule[depth][1]] = [goal[1]]
        # object variable binding
        if rule[depth][2] not in variable_substitution.keys():
            variable_substitution[rule[depth][2]] = [goal[2]]   
        depth += 1
        
    if depth == len(rule)-1:

        return proof_path_completion(rComp_substitution, rule)

    else :
        common_variable = []
        current_body = rule_structure[rule_structure['rule_number'] == rule_num].iloc[0,depth]
        current_variable = list(current_body.keys())
        common_variable = [variable for variable in current_variable if variable in variable_substitution]
        cVar_position = list(map(current_body.get, common_variable)) 
        unified_cVar= list(map(variable_substitution.get, common_variable))
        
        if len(common_variable) == 1:
            cVar_index = set(itertools.chain.from_iterable(
                             list(map(lambda x : KG_index.get(x).get(cVar_position[0]), unified_cVar[0]))))

            sub_goal = KG.loc[cVar_index]
            sub_goal.columns = [rule[depth][0], rule[depth][1], rule[depth][2]]
            rComp_substitution[rule[depth]] = sub_goal

            # subject variable binding
            if rule[depth][1] not in variable_substitution.keys():
                variable_substitution[rule[depth][1]] = list(set(sub_goal[rule[depth][1]].values))
            # object variable binding
            if rule[depth][2] not in variable_substitution.keys():
                variable_substitution[rule[depth][2]] = list(set(sub_goal[rule[depth][2]].values))

        else : 
            subj_cVar_index = set(itertools.chain.from_iterable(
                                  list(map(lambda x : KG_index.get(x).get(cVar_position[0]), unified_cVar[0]))))
            obj_cVar_index = set(itertools.chain.from_iterable(
                                  list(map(lambda x : KG_index.get(x).get(cVar_position[1]), unified_cVar[1]))))
            sub_goal = KG.loc[subj_cVar_index &obj_cVar_index]
            sub_goal.columns = [rule[depth][0], rule[depth][1], rule[depth][2]]
            rComp_substitution[rule[depth]] = sub_goal      
        depth += 1
       
        return unification(goal, rule, KG, KG_index, rule_num, depth, 
                           variable_substitution, rComp_substitution,rule_structure)

def backward_chaining(query, KG, KG_index, rules, rule_structure, sym2id_dict, neg_per_pos):
    '''
    - function: 
        1. Calls the unification function and performs backward chaining on the query triple
        2. return KG relation path, Rule template relatoin path, Maximum number of path 
  
    param:
    quert(DataFrame) -- query triples
        -Index(['pred', 'subj', 'obj'], dtype='object')
    KG(DataFarme) -- Knowledge Graph
        -Index(['pred', 'subj', 'obj'], dtype='object')
    KG_index(dictionary) -- a hash table that maps DataFrame index according to KG's entity information
    rules(list of list) -- all rule template 
        (e.g. [[('#1', 'X', 'Y'), ('#2', 'X', 'Z'), ('#3', 'Z', 'Y'), 3],
               [('#1', 'X', 'Y'), ('#2', 'X', 'Y'), 3]])
    rule_structure(DataFrame) -- a meta table with all rule template component information and rule numbers
    sym2id_dict(dictionary) -- a dictionary for mapping symbol-type relation to index
        (e.g. {'UNK': 0, 'PAD': 1, '#1_0_0': 2, ..., 'hasParent': 20, 'nationality': 21, ... })
    neg_per_pos(int) -- the number of negative proof path to be sampled per positive proof path

    return:
    relation_path -- Set of unified KG relation paths for all query triples
        (e.g. if given Query triple = [nationality BART, USA] and Rule template = [#1(X, Y) :- #2(X, Z), #3(Z, Y)]
              then KG relation path = ['nationaliy', 'birthPlace', 'locatedIn'])
    rule_temp_path -- Set of unified rule template relation paths for all query triples
        (e.g. if given Query triple = [nationality BART, USA] and Rule template = [#1(X, Y) :- #2(X, Z), #3(Z, Y)]
              then Rule template path = ['#1', '#2', '#3'])
    max_path(int) -- Maximum number of proof paths for all queries
    '''
    number = 0
    unify_dict = collections.defaultdict(set)##추가
    relation_path = []
    rule_temp_path = []
    max_path = 0
    for row in query.itertuples(index=False):
        number += 1
        if number%100 == 0:
            print('generate proof paths : '+str(number)+'/'+str(len(query)))
        elif number == len(query):
            print('complete generating proof paths! : '+str(number)+'/'+str(len(query)))
        goal = list(row)
        aug_rel_path_list = []
        aug_rule_temp_path_list = []
        for rule_num, rule in enumerate(rules):     
            unified_rel_path = unification(goal, rule, KG, KG_index,
                                                rule_num = rule_num, depth = 0, variable_substitution={}, 
                                                rComp_substitution={}, rule_structure = rule_structure)
            if len(unified_rel_path) > 0 :
                if max_path < len(unified_rel_path):
                    max_path = len(unified_rel_path)
                
                aug_rel_path = [[list(map(lambda x : sym2id_dict[x[1]], path))]*rule[-1] 
                                 for path in unified_rel_path]
                aug_rule_temp_path = [[list(map(lambda x : sym2id_dict[f'{x[0]}_{str(rule_num)}_{str(aug_num)}'], path)) 
                                      for aug_num in range(rule[-1])] for path in unified_rel_path]
                
                
                unified_rel_path = list(itertools.chain.from_iterable(unified_rel_path))
                for key, value in unified_rel_path:
                    unify_dict[f'{key}_{rule_num}'].add(value)
                
                aug_rel_path_list.append(aug_rel_path)
                aug_rule_temp_path_list.append(aug_rule_temp_path)
                
        relation_path.append(tuple(aug_rel_path_list))
        rule_temp_path.append(tuple(aug_rule_temp_path_list))

    return relation_path, rule_temp_path, max_path, unify_dict