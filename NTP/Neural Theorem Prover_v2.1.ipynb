{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Theorem Prover using pandas and Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Symbolic Unificaiton using pandas DataFrame\n",
    "- Load Files(Config, KG, Rule Template, ...)\n",
    "- Generate Meta Tables(Rule Structure, KG index, ...)\n",
    "- Run Backward Chaining and generate batch \n",
    "\n",
    "## 2. NTP Model Training with PyTorch\n",
    "- Define Model Structure using PyTorch\n",
    "- Define Foward Function \n",
    "- Training Model\n",
    "\n",
    "## 3. Extract Rules from Trained Embedding Vectors\n",
    "- Matching Rule templates with Embedding vectors \n",
    "- Extract Induced Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom functions\n",
    "from util.fileUtils import load_conf, load_from_file, create_directory\n",
    "from ntp.prover import backward_chaining\n",
    "from preprocess.dataPreprocessing import data_filter, padding\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from pprint import pprint\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# to print pandas dataframe\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_columns', 50)\n",
    "random.seed(1337)\n",
    "torch.manual_seed(1337)\n",
    "torch.cuda.manual_seed_all(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dataname options:\n",
    "    - example_8 / kinship / umls / nations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'example_8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'kg': './data/example_8.txt', 'templates': './data/example_8.nlt'},\n",
       " 'meta': {'result_dir': './out/example_8',\n",
       "  'result_file': '/example_8_rule.tsv'},\n",
       " 'training': {'num_epochs': 100,\n",
       "  'report_interval': 10,\n",
       "  'batch_size': 8,\n",
       "  'neg_per_pos': 2,\n",
       "  'learning_rate': 0.01,\n",
       "  'shuffle': True},\n",
       " 'model': {'embedding_size': 100,\n",
       "  'l2': 0.0001,\n",
       "  'drop_prob': 0.05,\n",
       "  'init': True}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = load_conf(f\"./config/{data_name}.conf\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_size = config['model']['embedding_size']\n",
    "drop_prob = config['model']['drop_prob']\n",
    "weight_decay = config['model']['l2']\n",
    "epochs = config['training']['num_epochs']\n",
    "report_interver_epoch = config['training']['report_interval']\n",
    "learning_rate = config['training']['learning_rate']\n",
    "neg_per_pos = config['training']['neg_per_pos']\n",
    "batch_size = config['training']['batch_size']\n",
    "init = config['model']['init']\n",
    "result_dir = config['meta']['result_dir']\n",
    "result_file = config['meta']['result_file']\n",
    "shuffle = config['training']['shuffle'] # True일 경우 매 epoch마다 데이터 random suffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data Files using pandas\n",
    "- KG : Knowledge Graph file with triple form\n",
    "- Query : query with triple form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "KG = pd.read_csv(config['data']['kg'], sep='\\t', names=['subj','pred','obj'])\n",
    "Query = pd.read_csv(config['data']['kg'], sep='\\t', names=['subj','pred','obj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>subj</th>\n",
       "      <th>obj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nationality</td>\n",
       "      <td>BART</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>birthPlace</td>\n",
       "      <td>BART</td>\n",
       "      <td>NEWYORK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>locatedIn</td>\n",
       "      <td>NEWYORK</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hasFather</td>\n",
       "      <td>BART</td>\n",
       "      <td>HOMER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hasGrandfather</td>\n",
       "      <td>BART</td>\n",
       "      <td>ABE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pred     subj      obj\n",
       "0     nationality     BART      USA\n",
       "1      birthPlace     BART  NEWYORK\n",
       "2       locatedIn  NEWYORK      USA\n",
       "3       hasFather     BART    HOMER\n",
       "4  hasGrandfather     BART      ABE"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KG = KG[['pred', 'subj', 'obj']]\n",
    "KG.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>subj</th>\n",
       "      <th>obj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nationality</td>\n",
       "      <td>BART</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>birthPlace</td>\n",
       "      <td>BART</td>\n",
       "      <td>NEWYORK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>locatedIn</td>\n",
       "      <td>NEWYORK</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hasFather</td>\n",
       "      <td>BART</td>\n",
       "      <td>HOMER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hasGrandfather</td>\n",
       "      <td>BART</td>\n",
       "      <td>ABE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pred     subj      obj\n",
       "0     nationality     BART      USA\n",
       "1      birthPlace     BART  NEWYORK\n",
       "2       locatedIn  NEWYORK      USA\n",
       "3       hasFather     BART    HOMER\n",
       "4  hasGrandfather     BART      ABE"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Query = Query[['pred', 'subj', 'obj']]\n",
    "Query.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_list = sorted(set(KG.subj.values).union(set(KG.obj.values)))\n",
    "len(entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting time :  0:00:00.000332\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "#KG index dictionary initializing\n",
    "KG_index = {}\n",
    "for entity in entity_list:\n",
    "    KG_index[entity] = {'subj':[], 'obj':[]}\n",
    "    \n",
    "subj_entities = KG['subj'].tolist()\n",
    "obj_entities = KG['obj'].tolist()\n",
    "\n",
    "#KG index dictionary generation\n",
    "for i in range(len(KG)):\n",
    "    KG_index[subj_entities[i]]['subj'] = KG_index.get(subj_entities[i]).get('subj')+[i]\n",
    "    KG_index[obj_entities[i]]['obj'] = KG_index.get(obj_entities[i]).get('obj')+[i]\n",
    "\n",
    "end = datetime.now() \n",
    "print('converting time : ', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ABE': {'subj': [], 'obj': [4, 5]},\n",
       " 'BART': {'subj': [0, 1, 3, 4, 7], 'obj': [6]},\n",
       " 'HOMER': {'subj': [5], 'obj': [3]},\n",
       " 'LISA': {'subj': [6], 'obj': [7]},\n",
       " 'NEWYORK': {'subj': [2], 'obj': [1]},\n",
       " 'USA': {'subj': [], 'obj': [0, 2]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KG_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Rule template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('#1', 'X', 'Y'), ('#2', 'X', 'Z'), ('#3', 'Z', 'Y'), 2],\n",
       " [('#1', 'X', 'Y'), ('#2', 'Y', 'X'), 2]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules, max_atom = load_from_file(config['data']['templates'])\n",
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>rule_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'X': 'subj', 'Y': 'obj'}</td>\n",
       "      <td>{'X': 'subj', 'Z': 'obj'}</td>\n",
       "      <td>{'Z': 'subj', 'Y': 'obj'}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'X': 'subj', 'Y': 'obj'}</td>\n",
       "      <td>{'Y': 'subj', 'X': 'obj'}</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0                          1  \\\n",
       "0  {'X': 'subj', 'Y': 'obj'}  {'X': 'subj', 'Z': 'obj'}   \n",
       "1  {'X': 'subj', 'Y': 'obj'}  {'Y': 'subj', 'X': 'obj'}   \n",
       "\n",
       "                           2  rule_number  \n",
       "0  {'Z': 'subj', 'Y': 'obj'}            0  \n",
       "1                       None            1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule_structure = pd.DataFrame(list(map(lambda x : [{atom[1]: 'subj', atom[2]: 'obj'} for atom in x[:-1]], rules)))\n",
    "rule_structure['rule_number'] = [i for i in range(len(rules))]\n",
    "rule_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dictionary from KG & Query data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "KG_predicate_list = sorted(set(KG.pred.values).union(set(Query.pred.values)))\n",
    "\n",
    "rule_pred_list = []\n",
    "for i, rule in enumerate(rules):\n",
    "    # iterate rule components\n",
    "    for r in rule[:-1]:\n",
    "        #iterate augmnet number\n",
    "        for j in range(rule[-1]):\n",
    "            suffix = '_' + str(i) + '_' + str(j)\n",
    "            rule_pred_list.append(r[0]+suffix)\n",
    "            \n",
    "predicate_list = sorted(set(KG_predicate_list).union(set(rule_pred_list)))\n",
    "# print('predicates : ',predicate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2sym_dict = {}\n",
    "sym2id_dict = {}\n",
    "sym2id_dict['UNK'] = 0\n",
    "sym2id_dict['PAD'] = 1\n",
    "id2sym_dict[0] = 'UNK'\n",
    "id2sym_dict[1] = 'PAD'\n",
    "\n",
    "\n",
    "for i, p in enumerate(predicate_list):\n",
    "    sym2id_dict[p] = i+2\n",
    "    id2sym_dict[i+2] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 0,\n",
       " 'PAD': 1,\n",
       " '#1_0_0': 2,\n",
       " '#1_0_1': 3,\n",
       " '#1_1_0': 4,\n",
       " '#1_1_1': 5,\n",
       " '#2_0_0': 6,\n",
       " '#2_0_1': 7,\n",
       " '#2_1_0': 8,\n",
       " '#2_1_1': 9,\n",
       " '#3_0_0': 10,\n",
       " '#3_0_1': 11,\n",
       " 'birthPlace': 12,\n",
       " 'hasFather': 13,\n",
       " 'hasGrandfather': 14,\n",
       " 'hasParent': 15,\n",
       " 'locatedIn': 16,\n",
       " 'nationality': 17,\n",
       " 'sibling': 18}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sym2id_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Backward Chaining\n",
    "\n",
    "#### unification\n",
    "- goal: query (e.g. nationality BART USA)\n",
    "- rule: rule template (e.g. #1(X,Y) :- #2(X,Z), #3(Z,Y))\n",
    "\n",
    "- 주어진 rule template의 conclusion과 query를 unify\n",
    "    - unify된 트리플은 rule component substitution에 key를 rule component(e.g. #1(X,Y))로  \n",
    "        value를 unified triples(dataframe)으로 저장   \n",
    "    \n",
    "        #1(X, Y) :\n",
    "    \n",
    "            |     pred    | subj | obj |\n",
    "            |-------------|------|-----|\n",
    "            | nationality | BART | USA |\n",
    "    \n",
    "    - conclusion의 X,Y와 같은 variable에 대하여 unify된 트리플을 참조하여 variable substitution에  \n",
    "    X : [BART], Y: [USA] 와 같이 binding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- 앞서 binding된 variable을 참조하여 각 rule body에 맞는 트리플을 unify\n",
    "    - #1(X,Y)를 통해 binding된 X에 대한 variable substitution을 참조하여 #2(X,Z)와 같은 body에 트리플을 unify하는 작업을 수행\n",
    "        - 위 경우에는 variable substitution을 참조하여 X가 subject인 트리플을 찾아 unify   \n",
    "    - unify된 트리플은 rule component substitution에 key를 rule component(e.g. #2(X,Y))로  \n",
    "        value를 unified triples(dataframe)으로 저장   \n",
    "           \n",
    "       #2(X, Z) :\n",
    "\n",
    "            |     pred     | subj | obj     |\n",
    "            |--------------|------|---------|\n",
    "            | placeOfBirth | BART | NEWYORK |\n",
    "            | hasFather    | BART | HOMMER  |    \n",
    "        \n",
    "    - 규칙 body의 X,Z와 같은 variable에 대하여 unify된 트리플을 참조하여 variable substitution에  \n",
    "    Z : [NEWYORK, HOMMER] 와 같이 binding\n",
    "    \n",
    "#### proof path completion\n",
    "- rule template을 분석하여 인접한 rule component간의 common variable 도출 \n",
    "- common variable을 기준으로 unified triple을 join하여 proof path를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete generating proof paths! : 8/8\n"
     ]
    }
   ],
   "source": [
    "relation_path, rule_temp_path, max_path = backward_chaining(Query, KG, KG_index, \n",
    "                                                            rules, rule_structure, sym2id_dict, neg_per_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data filtering\n",
    "- proof path가 없는 데이터 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_path = list(filter(data_filter, relation_path))\n",
    "rule_temp_path = list(filter(data_filter, rule_temp_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### padding\n",
    "- rule의 최대 구성 요소 수와 최대 proof path 수로 모든 proof path padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_path, rule_temp_path = padding(relation_path, rule_temp_path, rules, max_path, \n",
    "                                        max_atom, neg_per_pos, sym2id_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_tensor(path_data):\n",
    "    path_tensor = []\n",
    "    for i in path_data:\n",
    "        for j in i:\n",
    "            path_tensor.append(j)\n",
    "    path_tensor = torch.tensor(path_tensor)\n",
    "    return path_tensor\n",
    "\n",
    "relation_tensor = convert_list_to_tensor(relation_path)\n",
    "rule_temp_tensor = convert_list_to_tensor(rule_temp_path)\n",
    "\n",
    "answer = [1]\n",
    "for j in range(neg_per_pos):\n",
    "    answer += [0]\n",
    "answer = torch.tensor(answer, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class proof_path_dataset(Dataset): \n",
    "    def __init__(self, relation_tensor, rule_temp_tensor, label):\n",
    "        self.relation_tensor = relation_tensor\n",
    "        self.rule_temp_tensor = rule_temp_tensor\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.relation_tensor)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        rel_path = self.relation_tensor[idx]\n",
    "        rule_tamplate= self.rule_temp_tensor[idx]\n",
    "        label = self.label\n",
    "        return rel_path, rule_tamplate, label\n",
    "    \n",
    "\n",
    "dataset = proof_path_dataset(relation_tensor, rule_temp_tensor, answer)\n",
    "batch_generator = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTP(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size, max_path, dropout):\n",
    "        super(NTP, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding_matrix = nn.Embedding(self.vocab_size, self.embedding_size, padding_idx = sym2id_dict['PAD'])\n",
    "        self.max_path = max_path\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def RBF_kernel(self, embed_aug_rule_temp_path, embed_aug_rel_path):\n",
    "        L2_norm = torch.sqrt((embed_aug_rel_path - embed_aug_rule_temp_path).pow(2).sum(4))\n",
    "        sim = torch.exp(-L2_norm/2)\n",
    "        return sim\n",
    "    \n",
    "    def calculate_sim_avg(self, aug_rule_temp_path, aug_rel_path):\n",
    "\n",
    "        embed_aug_rule_temp_path = self.embedding_matrix(aug_rule_temp_path)\n",
    "        embed_aug_rel_path = self.embedding_matrix(aug_rel_path)\n",
    "                                                         \n",
    "        embed_aug_rule_temp_path = self.dropout(embed_aug_rule_temp_path)\n",
    "        embed_aug_rel_path = self.dropout(embed_aug_rel_path)\n",
    "        \n",
    "        sims=self.RBF_kernel(embed_aug_rule_temp_path, embed_aug_rel_path)\n",
    "        avg_sims = torch.mean(sims, 3)\n",
    "\n",
    "        return avg_sims\n",
    "        \n",
    "        \n",
    "    def forward(self, aug_rule_temp_path, aug_rel_path):\n",
    "        \n",
    "        avg_sims = self.calculate_sim_avg(aug_rule_temp_path, aug_rel_path)\n",
    "        max_sims = torch.max(avg_sims, axis=2)[0]\n",
    "        min_sims = torch.min(max_sims.view(-1, self.max_path), axis=1)[0]\n",
    "        \n",
    "        return min_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NTP(\n",
       "  (embedding_matrix): Embedding(19, 100, padding_idx=1)\n",
       "  (dropout): Dropout(p=0.05, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vocab_size = len(sym2id_dict)\n",
    "ntp = NTP(vocab_size, embedding_size, max_path, drop_prob)\n",
    "ntp.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "if init:\n",
    "    ntp.apply(initialize_weights); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Relation Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Time: 0:00:00.180884 \tEpoch: 10\tTotal Loss: 0.621 \tCurrent Loss: 0.541\n",
      "Epoch Time: 0:00:00.011256 \tEpoch: 20\tTotal Loss: 0.552 \tCurrent Loss: 0.440\n",
      "Epoch Time: 0:00:00.010340 \tEpoch: 30\tTotal Loss: 0.499 \tCurrent Loss: 0.368\n",
      "Epoch Time: 0:00:00.010247 \tEpoch: 40\tTotal Loss: 0.461 \tCurrent Loss: 0.338\n",
      "Epoch Time: 0:00:00.009947 \tEpoch: 50\tTotal Loss: 0.435 \tCurrent Loss: 0.326\n",
      "Epoch Time: 0:00:00.013355 \tEpoch: 60\tTotal Loss: 0.416 \tCurrent Loss: 0.305\n",
      "Epoch Time: 0:00:00.012854 \tEpoch: 70\tTotal Loss: 0.403 \tCurrent Loss: 0.309\n",
      "Epoch Time: 0:00:00.012741 \tEpoch: 80\tTotal Loss: 0.393 \tCurrent Loss: 0.322\n",
      "Epoch Time: 0:00:00.010659 \tEpoch: 90\tTotal Loss: 0.384 \tCurrent Loss: 0.317\n",
      "Epoch Time: 0:00:00.012505 \tEpoch: 100\tTotal Loss: 0.377 \tCurrent Loss: 0.331\n",
      "\n",
      "Total Training Time :  0:00:00.285095\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(ntp.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "ntp.train()\n",
    "epoch_loss = 0\n",
    "start_time = datetime.now()\n",
    "estart_time = datetime.now()\n",
    "for epoch in range(1, epochs+1):\n",
    "    for aug_rel_path, aug_rule_temp_path, label in batch_generator:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        label = torch.flatten(label).to(device)\n",
    "        \n",
    "        y_hat = ntp.forward(aug_rule_temp_path.to(device), aug_rel_path.to(device))\n",
    "        \n",
    "        loss = criterion(y_hat, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    if epoch%report_interver_epoch == 0:\n",
    "        end_time = datetime.now()\n",
    "        print(f'Epoch Time: {end_time-estart_time} \\tEpoch: {epoch}', end='')\n",
    "        print(f'\\tTotal Loss: {epoch_loss/epoch:.3f} \\tCurrent Loss: {loss.item():.3f}')\n",
    "        estart_time = end_time\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('\\nTotal Training Time : ', end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write rule file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representation_match(x, emb):\n",
    "    dist = torch.torch.nn.functional.pairwise_distance(x, emb)\n",
    "    sim = torch.exp(-dist)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-2.8460e-04, -6.1410e-04, -4.3693e-04,  ..., -2.5244e-04,\n",
      "         -2.8932e-05,  1.3444e-04],\n",
      "        [-3.1353e-04, -4.0521e-05, -7.6841e-05,  ..., -1.0415e-03,\n",
      "          2.9141e-05,  1.2764e-04],\n",
      "        [-1.4191e-01,  2.1010e-01, -5.4320e-01,  ..., -2.3453e-02,\n",
      "          1.4475e-01, -1.3630e-01],\n",
      "        ...,\n",
      "        [ 3.1612e-01,  1.8139e-01, -5.5350e-01,  ..., -4.6141e-01,\n",
      "          3.3765e-01, -6.6033e-02],\n",
      "        [-2.1487e-01,  4.9005e-02, -3.7651e-01,  ..., -2.8724e-01,\n",
      "          7.4786e-02, -7.3228e-02],\n",
      "        [-2.8372e-01, -6.2879e-02,  3.2541e-01,  ...,  2.3129e-01,\n",
      "         -1.8233e-01, -7.9255e-02]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#get trained embedding matrix\n",
    "for i in enumerate(ntp.parameters()):\n",
    "    print(i[1])\n",
    "    embeddings = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get parameterized rule template\n",
    "rule_templates = {}\n",
    "idx_rule_templates = {}\n",
    "for rule_number, template in enumerate(rules):\n",
    "    result_template_key = []\n",
    "    ids_result_template_value = []\n",
    "    ids_result_template_values = []\n",
    "    for i in range(len(template)-1):\n",
    "        rule_element=(f'p{int(template[i][0][1])-1}_{rule_number}', template[i][1], template[i][2])       \n",
    "        result_template_key.append(rule_element)\n",
    "        rule_element = ()\n",
    "\n",
    "    for aug in range(template[-1]):\n",
    "        for j in range(len(template)-1):\n",
    "            ids_result_template_value.append([sym2id_dict[template[j][0]+'_'+str(rule_number)+'_'+\n",
    "                                                           str(aug)], template[j][1], template[j][2]])\n",
    "        ids_result_template_values.append(ids_result_template_value)\n",
    "        ids_result_template_value = []\n",
    "    idx_rule_templates[tuple(result_template_key)] = ids_result_template_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get rule instance & write rule file\n",
    "\n",
    "# 자기 자신을 masking하기 위한 rule template의 relation index생성\n",
    "masking_index = []\n",
    "for key, rules in idx_rule_templates.items():\n",
    "    for rule in rules:\n",
    "        for element in rule:\n",
    "            masking_index.append(element[0])\n",
    "            \n",
    "create_directory(result_dir)\n",
    "with open(result_dir + result_file, 'w') as f:\n",
    "    for key, rules in idx_rule_templates.items():\n",
    "        result = []\n",
    "        f.write(str(key)+'\\n')\n",
    "        for rule in rules:\n",
    "            relation_similarities = []\n",
    "            rule_result = []\n",
    "            for element in rule:\n",
    "                masking_index = masking_index+[element[0]]+[0, 1]\n",
    "                x = ntp.embedding_matrix(torch.tensor([element[0]]).to(device))\n",
    "                match = representation_match(x, embeddings)\n",
    "                match[masking_index] = 0\n",
    "                top_k = torch.topk(match, 1)\n",
    "                rule_result.append(id2sym_dict[top_k.indices.item()]+'('+element[1]+','+element[2]+')')\n",
    "                relation_similarities.append(match[top_k.indices])\n",
    "            confidence_score = str(min(relation_similarities).item())\n",
    "\n",
    "            head = rule_result[0]\n",
    "            body = rule_result[1:]\n",
    "            \n",
    "            result.append((round(min(relation_similarities).item(), 6), head + ' :- ' +\", \".join(body)+'\\n'))\n",
    "            result.sort(reverse = True)\n",
    "        for score, rule in result:\n",
    "            f.write(str(score)+ '\\t' + rule)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
