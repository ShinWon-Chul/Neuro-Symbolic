{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Theorem Prover using pandas and Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Symbolic Unificaiton using pandas DataFrame\n",
    "- Load Files(Config, KG, Rule Template, ...)\n",
    "- Generate Meta Tables(Rule Structure, KG index, ...)\n",
    "- Run Backward Chaining and generate batch \n",
    "\n",
    "## 2. NTP Model Training with PyTorch\n",
    "- Define Model Structure using PyTorch\n",
    "- Define Foward Function \n",
    "- Training Model\n",
    "\n",
    "## 3. Extract Rules from Trained Embedding Vectors\n",
    "- Matching Rule templates with Embedding vectors \n",
    "- Extract Induced Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom functions\n",
    "from util.fileUtils import load_conf, load_from_file, create_directory\n",
    "from ntp.prover import backward_chaining\n",
    "from preprocess.dataPreprocessing import data_filter, padding, proof_path_dataset\n",
    "from preprocess.dataPreprocessing import convert_list_to_tensor, negative_samplig\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from pprint import pprint\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# to print pandas dataframe\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "random.seed(1337)\n",
    "torch.manual_seed(1337)\n",
    "torch.cuda.manual_seed_all(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Config File\n",
    "- dataname options:\n",
    "    - example_8 / kinship / umls / nations\n",
    "  \n",
    "- parameters config:\n",
    "    - data\n",
    "        - kg : 지식 그래프 파일 위치\n",
    "        - templates : 규칙 템플릿 파일 위치\n",
    "    - meta\n",
    "        - result_dir : 학습 결과인 규칙 파일이 저장될 디렉터리\n",
    "        - result_file : 저장될 규칙 파일 이름\n",
    "    - training\n",
    "        - num_epochs : 학습 epoch 횟수\n",
    "        - report_interval : 학습시 log를 report할 iteration 주기\n",
    "        - batch_size : mini batch size\n",
    "        - neg_per_pos : positive data 하나당 생성될 negative data수\n",
    "        - learning_rate : learning rate\n",
    "        - shuffle(True/False) : epoch마다 training data를 random shuffle할지 여부\n",
    "    - model\n",
    "        - embedding size : symbol의 vector representation size\n",
    "        - l2 : weight decay rate (L2 penalty)\n",
    "        - drop_porb : dropout probability\n",
    "        - init(True/False) : \n",
    "            - True - xavier uniform distribution으로 embedding matrix 초기화  \n",
    "            - Flase - 평균0 분산1인 normal distribution으로 embedding matrix 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'example_8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'kg': './data/example_8.txt', 'templates': './data/example_8.nlt'},\n",
       " 'meta': {'result_dir': './out/example_8',\n",
       "  'result_file': '/example_8_rule.tsv'},\n",
       " 'training': {'num_epochs': 300,\n",
       "  'report_interval': 10,\n",
       "  'batch_size': 2,\n",
       "  'neg_per_pos': 1,\n",
       "  'learning_rate': 0.001,\n",
       "  'shuffle': True},\n",
       " 'model': {'embedding_size': 100,\n",
       "  'l2': 0.0001,\n",
       "  'drop_prob': 0.05,\n",
       "  'init': True}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = load_conf(f\"./config/{data_name}.conf\")\n",
    "\n",
    "embedding_size = config['model']['embedding_size']\n",
    "drop_prob = config['model']['drop_prob']\n",
    "weight_decay = config['model']['l2']\n",
    "epochs = config['training']['num_epochs']\n",
    "report_interver_epoch = config['training']['report_interval']\n",
    "learning_rate = config['training']['learning_rate']\n",
    "neg_per_pos = config['training']['neg_per_pos']\n",
    "batch_size = config['training']['batch_size']\n",
    "init = config['model']['init']\n",
    "result_dir = config['meta']['result_dir']\n",
    "result_file = config['meta']['result_file']\n",
    "shuffle = config['training']['shuffle']\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data Files using pandas\n",
    "- KG : Knowledge Graph file with triple form\n",
    "- Query : query with triple form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "KG = pd.read_csv(config['data']['kg'], sep='\\t', names=['subj','pred','obj'])\n",
    "Query = pd.read_csv(config['data']['kg'], sep='\\t', names=['subj','pred','obj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>subj</th>\n",
       "      <th>obj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nationality</td>\n",
       "      <td>BART</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>birthPlace</td>\n",
       "      <td>BART</td>\n",
       "      <td>NEWYORK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>locatedIn</td>\n",
       "      <td>NEWYORK</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hasFather</td>\n",
       "      <td>BART</td>\n",
       "      <td>HOMER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hasGrandfather</td>\n",
       "      <td>BART</td>\n",
       "      <td>ABE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pred     subj      obj\n",
       "0     nationality     BART      USA\n",
       "1      birthPlace     BART  NEWYORK\n",
       "2       locatedIn  NEWYORK      USA\n",
       "3       hasFather     BART    HOMER\n",
       "4  hasGrandfather     BART      ABE"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KG = KG[['pred', 'subj', 'obj']]\n",
    "KG.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>subj</th>\n",
       "      <th>obj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nationality</td>\n",
       "      <td>BART</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>birthPlace</td>\n",
       "      <td>BART</td>\n",
       "      <td>NEWYORK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>locatedIn</td>\n",
       "      <td>NEWYORK</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hasFather</td>\n",
       "      <td>BART</td>\n",
       "      <td>HOMER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hasGrandfather</td>\n",
       "      <td>BART</td>\n",
       "      <td>ABE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pred     subj      obj\n",
       "0     nationality     BART      USA\n",
       "1      birthPlace     BART  NEWYORK\n",
       "2       locatedIn  NEWYORK      USA\n",
       "3       hasFather     BART    HOMER\n",
       "4  hasGrandfather     BART      ABE"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Query = Query[['pred', 'subj', 'obj']]\n",
    "Query.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_list = sorted(set(KG.subj.values).union(set(KG.obj.values)))\n",
    "len(entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting time :  0:00:00.000528\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "#KG index dictionary initializing\n",
    "KG_index = {}\n",
    "for entity in entity_list:\n",
    "    KG_index[entity] = {'subj':[], 'obj':[]}\n",
    "    \n",
    "subj_entities = KG['subj'].tolist()\n",
    "obj_entities = KG['obj'].tolist()\n",
    "\n",
    "#KG index dictionary generation\n",
    "for i in range(len(KG)):\n",
    "    KG_index[subj_entities[i]]['subj'] = KG_index.get(subj_entities[i]).get('subj')+[i]\n",
    "    KG_index[obj_entities[i]]['obj'] = KG_index.get(obj_entities[i]).get('obj')+[i]\n",
    "\n",
    "end = datetime.now() \n",
    "print('converting time : ', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ABE': {'subj': [], 'obj': [4, 5]},\n",
       " 'BART': {'subj': [0, 1, 3, 4, 7], 'obj': [6]},\n",
       " 'HOMER': {'subj': [5], 'obj': [3]},\n",
       " 'LISA': {'subj': [6], 'obj': [7]},\n",
       " 'NEWYORK': {'subj': [2], 'obj': [1]},\n",
       " 'USA': {'subj': [], 'obj': [0, 2]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KG_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Rule template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('#1', 'X', 'Y'), ('#2', 'X', 'Z'), ('#3', 'Z', 'Y'), 2],\n",
       " [('#1', 'X', 'Y'), ('#2', 'Y', 'X'), 2]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules, max_atom = load_from_file(config['data']['templates'])\n",
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>rule_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'X': 'subj', 'Y': 'obj'}</td>\n",
       "      <td>{'X': 'subj', 'Z': 'obj'}</td>\n",
       "      <td>{'Z': 'subj', 'Y': 'obj'}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'X': 'subj', 'Y': 'obj'}</td>\n",
       "      <td>{'Y': 'subj', 'X': 'obj'}</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0                          1  \\\n",
       "0  {'X': 'subj', 'Y': 'obj'}  {'X': 'subj', 'Z': 'obj'}   \n",
       "1  {'X': 'subj', 'Y': 'obj'}  {'Y': 'subj', 'X': 'obj'}   \n",
       "\n",
       "                           2  rule_number  \n",
       "0  {'Z': 'subj', 'Y': 'obj'}            0  \n",
       "1                       None            1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule_structure = pd.DataFrame(list(map(lambda x : [{atom[1]: 'subj', atom[2]: 'obj'} for atom in x[:-1]], rules)))\n",
    "rule_structure['rule_number'] = [i for i in range(len(rules))]\n",
    "rule_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dictionary from KG & Query data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "KG_predicate_list = sorted(set(KG.pred.values).union(set(Query.pred.values)))\n",
    "\n",
    "rule_pred_list = []\n",
    "for i, rule in enumerate(rules):\n",
    "    # iterate rule components\n",
    "    for r in rule[:-1]:\n",
    "        #iterate augmnet number\n",
    "        for j in range(rule[-1]):\n",
    "            suffix = '_' + str(i) + '_' + str(j)\n",
    "            rule_pred_list.append(r[0]+suffix)\n",
    "            \n",
    "predicate_list = sorted(set(KG_predicate_list).union(set(rule_pred_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2sym_dict = {}\n",
    "sym2id_dict = {}\n",
    "sym2id_dict['UNK'] = 0\n",
    "sym2id_dict['PAD'] = 1\n",
    "id2sym_dict[0] = 'UNK'\n",
    "id2sym_dict[1] = 'PAD'\n",
    "\n",
    "for i, p in enumerate(predicate_list):\n",
    "    sym2id_dict[p] = i+2\n",
    "    id2sym_dict[i+2] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 0,\n",
       " 'PAD': 1,\n",
       " '#1_0_0': 2,\n",
       " '#1_0_1': 3,\n",
       " '#1_1_0': 4,\n",
       " '#1_1_1': 5,\n",
       " '#2_0_0': 6,\n",
       " '#2_0_1': 7,\n",
       " '#2_1_0': 8,\n",
       " '#2_1_1': 9,\n",
       " '#3_0_0': 10,\n",
       " '#3_0_1': 11,\n",
       " 'birthPlace': 12,\n",
       " 'hasFather': 13,\n",
       " 'hasGrandfather': 14,\n",
       " 'hasParent': 15,\n",
       " 'locatedIn': 16,\n",
       " 'nationality': 17,\n",
       " 'sibling': 18}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sym2id_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Backward Chaining\n",
    "\n",
    "#### 1. unification\n",
    "- goal: query (e.g. nationality BART USA)\n",
    "- rule: rule template (e.g. #1(X,Y) :- #2(X,Z), #3(Z,Y))\n",
    "\n",
    "- 주어진 rule template의 conclusion과 query를 unify\n",
    "    - unify된 트리플은 다음과 같이 `rule component substitution`에 key를 rule component(e.g. #1(X,Y))로  \n",
    "        value를 unified triples(dataframe)으로 저장   \n",
    "    \n",
    "        #1(X, Y) :\n",
    "    \n",
    "            |     pred    | subj | obj |\n",
    "            |-------------|------|-----|\n",
    "            | nationality | BART | USA |\n",
    "    \n",
    "    - conclusion의 X,Y와 같은 variable에 대하여 unify된 트리플을 참조하여 `variable substitution`에 binding  \n",
    "        - (e.g. `variable substitution` = X : [BART], Y: [USA]) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- 앞서 binding된 variable을 참조하여 각 rule body에 맞는 트리플을 unify\n",
    "    - #1(X,Y)를 통해 binding된 X에 대한 `variable substitution`을 참조하여 #2(X,Z)와 같은 body에 트리플을 unify하는 작업을 수행\n",
    "        - #2(X,Z)의 경우에는 `variable substitution`을 참조하여 X(BART)가 subject인 트리플을 찾아 unify   \n",
    "    - unify된 트리플은 다음과 같이 `rule component substitution`에 key를 rule component(e.g. #2(X,Y))로  \n",
    "        value를 unified triples(dataframe)으로 저장   \n",
    "           \n",
    "       #2(X, Z) :\n",
    "\n",
    "            |     pred     | subj | obj     |\n",
    "            |--------------|------|---------|\n",
    "            | placeOfBirth | BART | NEWYORK |\n",
    "            | hasFather    | BART | HOMMER  |    \n",
    "        \n",
    "    - 규칙 body의 Z와 같은 variable에 대하여 unify된 트리플을 참조하여 `variable substitution`에 binding  \n",
    "        - (e.g. `variable substitution` = Z : [NEWYORK, HOMMER], X : [BART], Y: [USA])\n",
    "    \n",
    "#### 2. proof path completion\n",
    "- rule template을 분석하여 인접한 rule component간의 common variable 도출 \n",
    "- common variable을 기준으로 unified triple을 join하여 proof path 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete generating proof paths! : 8/8\n"
     ]
    }
   ],
   "source": [
    "relation_path, rule_template_path, max_path, unify_dict = backward_chaining(Query, KG, KG_index, \n",
    "                                                                        rules, rule_structure, \n",
    "                                                                        sym2id_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data filtering\n",
    "- proof path가 없는 데이터 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_path = list(filter(data_filter, relation_path))\n",
    "rule_template_path = list(filter(data_filter, rule_template_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete negative sampling! : 4/4\n"
     ]
    }
   ],
   "source": [
    "KG_relation = set(KG['pred'])\n",
    "augment_num = rules[0][-1]\n",
    "\n",
    "for idx, (relation, rule_template) in enumerate(zip(relation_path, rule_template_path)):\n",
    "    if (idx+1) % 100 == 0:\n",
    "        print('negative sampling... : '+str(idx+1)+'/'+str(len(relation_path)))\n",
    "    elif (idx+1) == len(relation_path):\n",
    "        print('complete negative sampling! : '+str(idx+1)+'/'+str(len(relation_path)))\n",
    "    pos_neg_relation_path = []\n",
    "    pos_neg_rule_template = []\n",
    "    for rel, rule in zip(relation, rule_template):\n",
    "        rel_result, rule_result = negative_samplig(rel, rule, KG_relation, augment_num, \n",
    "                                                   neg_per_pos, sym2id_dict, id2sym_dict, unify_dict)\n",
    "        pos_neg_relation_path.append(rel_result)\n",
    "        pos_neg_rule_template.append(rule_result)\n",
    "    relation_path[idx] = tuple(pos_neg_relation_path)\n",
    "    rule_template_path[idx] = tuple(pos_neg_rule_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "- rule의 최대 구성 요소 수와 최대 proof path 수로 모든 proof path padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_path, rule_template_path = padding(relation_path, rule_template_path, rules, max_path, \n",
    "                                            max_atom, neg_per_pos, sym2id_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_tensor = convert_list_to_tensor(relation_path)\n",
    "rule_template_tensor = convert_list_to_tensor(rule_template_path)\n",
    "answer = [1]\n",
    "for j in range(neg_per_pos):\n",
    "    answer += [0]\n",
    "answer = torch.tensor(answer, dtype=torch.float32)\n",
    "augment_num = rules[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = proof_path_dataset(relation_tensor, rule_template_tensor, answer)\n",
    "batch_generator = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTP(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size, max_path, dropout):\n",
    "        super(NTP, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding_matrix = nn.Embedding(self.vocab_size, self.embedding_size, \n",
    "                                             padding_idx = sym2id_dict['PAD'])\n",
    "        self.max_path = max_path\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def RBF_kernel(self, embed_rule_template_path, embed_relation_path):\n",
    "        L2_norm = torch.sqrt((embed_relation_path - embed_rule_template_path).pow(2).sum(4))\n",
    "        similarity = torch.exp(-L2_norm/2)\n",
    "        return similarity\n",
    "    \n",
    "    def calculate_sim_avg(self, rule_template_path, relation_path):\n",
    "\n",
    "        embed_rule_template_path = self.embedding_matrix(rule_template_path)\n",
    "        embed_relation_path = self.embedding_matrix(relation_path)\n",
    "                                                         \n",
    "        embed_rule_template_path = self.dropout(embed_rule_template_path)\n",
    "        embed_relation_path = self.dropout(embed_relation_path)\n",
    "        sims=self.RBF_kernel(embed_rule_template_path, embed_relation_path)\n",
    "        avg_sims = torch.mean(sims, 3)\n",
    "        return avg_sims\n",
    "        \n",
    "        \n",
    "    def forward(self, rule_template_path, relation_path):\n",
    "        \n",
    "        avg_sims = self.calculate_sim_avg(rule_template_path, relation_path)\n",
    "        max_sims = torch.max(avg_sims, axis=2)[0]\n",
    "        min_sims = torch.min(max_sims.view(-1, self.max_path), axis=1)[0]\n",
    "        \n",
    "        return min_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NTP(\n",
       "  (embedding_matrix): Embedding(19, 100, padding_idx=1)\n",
       "  (dropout): Dropout(p=0.05, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vocab_size = len(sym2id_dict)\n",
    "ntp = NTP(vocab_size, embedding_size, max_path, drop_prob)\n",
    "ntp.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "if init:\n",
    "    ntp.apply(initialize_weights); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Relation Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Time: 0:00:00.129117 \tEpoch: 10\tTotal Loss: 1.392 \tCurrent Loss: 0.691\n",
      "Epoch Time: 0:00:00.015880 \tEpoch: 20\tTotal Loss: 1.352 \tCurrent Loss: 0.619\n",
      "Epoch Time: 0:00:00.017425 \tEpoch: 30\tTotal Loss: 1.308 \tCurrent Loss: 0.565\n",
      "Epoch Time: 0:00:00.016141 \tEpoch: 40\tTotal Loss: 1.264 \tCurrent Loss: 0.536\n",
      "Epoch Time: 0:00:00.017765 \tEpoch: 50\tTotal Loss: 1.221 \tCurrent Loss: 0.534\n",
      "Epoch Time: 0:00:00.013196 \tEpoch: 60\tTotal Loss: 1.179 \tCurrent Loss: 0.459\n",
      "Epoch Time: 0:00:00.012218 \tEpoch: 70\tTotal Loss: 1.139 \tCurrent Loss: 0.444\n",
      "Epoch Time: 0:00:00.012362 \tEpoch: 80\tTotal Loss: 1.101 \tCurrent Loss: 0.377\n",
      "Epoch Time: 0:00:00.012720 \tEpoch: 90\tTotal Loss: 1.065 \tCurrent Loss: 0.352\n",
      "Epoch Time: 0:00:00.014070 \tEpoch: 100\tTotal Loss: 1.030 \tCurrent Loss: 0.372\n",
      "Epoch Time: 0:00:00.012618 \tEpoch: 110\tTotal Loss: 0.999 \tCurrent Loss: 0.370\n",
      "Epoch Time: 0:00:00.012330 \tEpoch: 120\tTotal Loss: 0.971 \tCurrent Loss: 0.343\n",
      "Epoch Time: 0:00:00.012635 \tEpoch: 130\tTotal Loss: 0.946 \tCurrent Loss: 0.337\n",
      "Epoch Time: 0:00:00.013182 \tEpoch: 140\tTotal Loss: 0.923 \tCurrent Loss: 0.335\n",
      "Epoch Time: 0:00:00.012246 \tEpoch: 150\tTotal Loss: 0.901 \tCurrent Loss: 0.328\n",
      "Epoch Time: 0:00:00.016117 \tEpoch: 160\tTotal Loss: 0.882 \tCurrent Loss: 0.255\n",
      "Epoch Time: 0:00:00.024136 \tEpoch: 170\tTotal Loss: 0.863 \tCurrent Loss: 0.305\n",
      "Epoch Time: 0:00:00.015624 \tEpoch: 180\tTotal Loss: 0.847 \tCurrent Loss: 0.247\n",
      "Epoch Time: 0:00:00.013507 \tEpoch: 190\tTotal Loss: 0.832 \tCurrent Loss: 0.246\n",
      "Epoch Time: 0:00:00.013234 \tEpoch: 200\tTotal Loss: 0.818 \tCurrent Loss: 0.252\n",
      "Epoch Time: 0:00:00.012494 \tEpoch: 210\tTotal Loss: 0.805 \tCurrent Loss: 0.298\n",
      "Epoch Time: 0:00:00.012387 \tEpoch: 220\tTotal Loss: 0.793 \tCurrent Loss: 0.252\n",
      "Epoch Time: 0:00:00.012266 \tEpoch: 230\tTotal Loss: 0.782 \tCurrent Loss: 0.288\n",
      "Epoch Time: 0:00:00.012984 \tEpoch: 240\tTotal Loss: 0.771 \tCurrent Loss: 0.235\n",
      "Epoch Time: 0:00:00.012423 \tEpoch: 250\tTotal Loss: 0.761 \tCurrent Loss: 0.310\n",
      "Epoch Time: 0:00:00.012492 \tEpoch: 260\tTotal Loss: 0.751 \tCurrent Loss: 0.212\n",
      "Epoch Time: 0:00:00.012212 \tEpoch: 270\tTotal Loss: 0.743 \tCurrent Loss: 0.275\n",
      "Epoch Time: 0:00:00.012063 \tEpoch: 280\tTotal Loss: 0.734 \tCurrent Loss: 0.231\n",
      "Epoch Time: 0:00:00.012231 \tEpoch: 290\tTotal Loss: 0.726 \tCurrent Loss: 0.221\n",
      "Epoch Time: 0:00:00.012719 \tEpoch: 300\tTotal Loss: 0.719 \tCurrent Loss: 0.217\n",
      "\n",
      "Total Training Time :  0:00:00.531083\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(ntp.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "ntp.train()\n",
    "epoch_loss = 0\n",
    "start_time = datetime.now()\n",
    "estart_time = datetime.now()\n",
    "for epoch in range(1, epochs+1):\n",
    "    for relation_path, rule_template_path, label in batch_generator:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        label = torch.flatten(label).to(device)\n",
    "        \n",
    "        y_hat = ntp.forward(rule_template_path.to(device), relation_path.to(device))\n",
    "        \n",
    "        loss = criterion(y_hat, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    if epoch%report_interver_epoch == 0:\n",
    "        end_time = datetime.now()\n",
    "        print(f'Epoch Time: {end_time-estart_time} \\tEpoch: {epoch}', end='')\n",
    "        print(f'\\tTotal Loss: {epoch_loss/epoch:.3f} \\tCurrent Loss: {loss.item():.3f}')\n",
    "        estart_time = end_time\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('\\nTotal Training Time : ', end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write rule(result) file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representation_match(x, emb):\n",
    "    dist = torch.torch.nn.functional.pairwise_distance(x, emb)\n",
    "    sim = torch.exp(-dist)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.7536e-11, -7.4200e-07, -7.2798e-13,  ..., -1.4443e-07,\n",
      "          3.5233e-08,  1.3768e-15],\n",
      "        [ 2.1466e-13, -8.0051e-18, -1.2674e-15,  ...,  2.8015e-04,\n",
      "         -7.4563e-19,  1.9365e-15],\n",
      "        [ 1.2429e-01,  1.4468e-01,  3.2746e-02,  ...,  2.0227e-01,\n",
      "          1.2036e-01, -2.7266e-01],\n",
      "        ...,\n",
      "        [ 9.6702e-02,  8.7594e-02,  8.2490e-02,  ..., -1.6670e-01,\n",
      "          1.5226e-01,  4.6534e-02],\n",
      "        [-3.6866e-02,  6.5228e-03,  1.3013e-04,  ..., -4.0019e-07,\n",
      "          3.5717e-02, -2.8456e-02],\n",
      "        [-1.4886e-02, -1.7472e-02,  3.3357e-02,  ..., -3.3902e-02,\n",
      "          6.2160e-02,  2.1144e-02]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#get trained embedding matrix\n",
    "for i in enumerate(ntp.parameters()):\n",
    "    print(i[1])\n",
    "    embeddings = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get parameterized rule template\n",
    "rule_templates = {}\n",
    "idx_rule_templates = {}\n",
    "for rule_number, template in enumerate(rules):\n",
    "    result_template_key = []\n",
    "    ids_result_template_value = []\n",
    "    ids_result_template_values = []\n",
    "    for i in range(len(template)-1):\n",
    "        rule_element=(f'p{int(template[i][0][1])-1}_{rule_number}', template[i][1], template[i][2])       \n",
    "        result_template_key.append(rule_element)\n",
    "        rule_element = ()\n",
    "\n",
    "    for aug in range(template[-1]):\n",
    "        for j in range(len(template)-1):\n",
    "            ids_result_template_value.append([sym2id_dict[template[j][0]+'_'+str(rule_number)+'_'+\n",
    "                                                           str(aug)], template[j][1], template[j][2]])\n",
    "        ids_result_template_values.append(ids_result_template_value)\n",
    "        ids_result_template_value = []\n",
    "    idx_rule_templates[tuple(result_template_key)] = ids_result_template_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get rule instance & write rule file\n",
    "\n",
    "# 자기 자신을 masking하기 위한 rule template의 relation index생성\n",
    "masking_index = []\n",
    "for key, rules in idx_rule_templates.items():\n",
    "    for rule in rules:\n",
    "        for element in rule:\n",
    "            masking_index.append(element[0])\n",
    "            \n",
    "create_directory(result_dir)\n",
    "with open(result_dir + result_file, 'w') as f:\n",
    "    for key, rules in idx_rule_templates.items():\n",
    "        result = []\n",
    "        f.write(str(key)+'\\n')\n",
    "        for rule in rules:\n",
    "            relation_similarities = []\n",
    "            rule_result = []\n",
    "            for element in rule:\n",
    "                masking_index = masking_index+[element[0]]+[0, 1]\n",
    "                x = ntp.embedding_matrix(torch.tensor([element[0]]).to(device))\n",
    "                match = representation_match(x, embeddings)\n",
    "                match[masking_index] = 0\n",
    "                top_k = torch.topk(match, 1)\n",
    "                rule_result.append(id2sym_dict[top_k.indices.item()]+'('+element[1]+','+element[2]+')')\n",
    "                relation_similarities.append(match[top_k.indices])\n",
    "            confidence_score = str(min(relation_similarities).item())\n",
    "\n",
    "            head = rule_result[0]\n",
    "            body = rule_result[1:]\n",
    "            \n",
    "            result.append((round(min(relation_similarities).item(), 6), head + ' :- ' +\", \".join(body)+'\\n'))\n",
    "            result.sort(reverse = True)\n",
    "        for score, rule in result:\n",
    "            f.write(str(score)+ '\\t' + rule)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
