{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Theorem Prover using pandas and Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Symbolic Unificaiton using pandas DataFrame\n",
    "- Load Files \n",
    "- Define Functions \n",
    "- Generate Meta Tables\n",
    "- Run Symbolic Unification and generate batch \n",
    "\n",
    "## 2. NTP Model Training with PyTorch\n",
    "- Define Model Structure using PyTorch\n",
    "- Define Foward Function \n",
    "- Training Model\n",
    "\n",
    "## 3. Extract Rules from Trained Embedding Vectors\n",
    "- Matching Rule templates with Embedding vectors \n",
    "- Extract Induced Rules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "import copy\n",
    "from pprint import pprint\n",
    "# from itertools import permutations\n",
    "from datetime import datetime, timedelta\n",
    "from collections.abc import Iterable\n",
    "# from itertools import combinations\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "\n",
    "# to print pandas dataframe\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Symbolic Unificaiton using pandas DataFrame\n",
    "### Load Data Files using pandas\n",
    "- KG : Knowledge Graph file with triple form\n",
    "- Query : query with triple form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_name options = example_7, kinship, umls, nations\n",
    "data_name = 'example_7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size :  3\n"
     ]
    }
   ],
   "source": [
    "pos_per_batch = 1 #Number of positive datas to be included in one batch\n",
    "neg_per_pos = 2 #The number of negative data to be sampled per positive data\n",
    "batch_size = pos_per_batch + (pos_per_batch * neg_per_pos)\n",
    "print('batch_size : ', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "KG = pd.read_csv(f'./data/{data_name}.txt', sep='\\t', names=['subj','pred','obj'])\n",
    "Query = pd.read_csv(f'./data/{data_name}.txt', sep='\\t', names=['subj','pred','obj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>subj</th>\n",
       "      <th>obj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nationality</td>\n",
       "      <td>BART</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>placeOfBirth</td>\n",
       "      <td>BART</td>\n",
       "      <td>NEWYORK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>locatedIn</td>\n",
       "      <td>NEWYORK</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hasFather</td>\n",
       "      <td>BART</td>\n",
       "      <td>HOMMER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nationality</td>\n",
       "      <td>HOMMER</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pred     subj      obj\n",
       "0   nationality     BART      USA\n",
       "1  placeOfBirth     BART  NEWYORK\n",
       "2     locatedIn  NEWYORK      USA\n",
       "3     hasFather     BART   HOMMER\n",
       "4   nationality   HOMMER      USA"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KG = KG[['pred', 'subj', 'obj']]\n",
    "KG.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>subj</th>\n",
       "      <th>obj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nationality</td>\n",
       "      <td>BART</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hasFather</td>\n",
       "      <td>BART</td>\n",
       "      <td>HOMMER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>locatedIn</td>\n",
       "      <td>NEWYORK</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>placeOfBirth</td>\n",
       "      <td>BART</td>\n",
       "      <td>NEWYORK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>placeOfBirth</td>\n",
       "      <td>HOMMER</td>\n",
       "      <td>NEWYORK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pred     subj      obj\n",
       "0   nationality     BART      USA\n",
       "1     hasFather     BART   HOMMER\n",
       "2     locatedIn  NEWYORK      USA\n",
       "3  placeOfBirth     BART  NEWYORK\n",
       "4  placeOfBirth   HOMMER  NEWYORK"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Query = Query.sample(frac=1).reset_index(drop=True)\n",
    "Query = Query[['pred', 'subj', 'obj']]\n",
    "Query.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_list = sorted(set(KG.subj.values).union(set(KG.obj.values)))\n",
    "len(entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting time :  0:00:00.001000\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "#KG index dictionary initializing\n",
    "KG_index = {}\n",
    "for entity in entity_list:\n",
    "    KG_index[entity] = {'subj':[], 'obj':[]}\n",
    "    \n",
    "subj_entities = KG['subj'].tolist()\n",
    "obj_entities = KG['obj'].tolist()\n",
    "\n",
    "#KG index dictionary generation\n",
    "for i in range(len(KG)):\n",
    "    KG_index[subj_entities[i]]['subj'] = KG_index.get(subj_entities[i]).get('subj')+[i]\n",
    "    KG_index[obj_entities[i]]['obj'] = KG_index.get(obj_entities[i]).get('obj')+[i]\n",
    "\n",
    "end = datetime.now() \n",
    "print('converting time : ', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BART': {'subj': [0, 1, 3, 6], 'obj': []},\n",
       " 'HOMMER': {'subj': [4, 5], 'obj': [3]},\n",
       " 'NEWYORK': {'subj': [2], 'obj': [1, 5]},\n",
       " 'USA': {'subj': [], 'obj': [0, 2, 4, 6]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KG_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Rule template and parsing using regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(string):\n",
    "    \"\"\"\n",
    "    - function: trim whitespaces\n",
    "    :param string: an input string\n",
    "    \n",
    "    :return: the string without trailing whitespaces\n",
    "    \"\"\"\n",
    "    return re.sub(\"\\A\\s+|\\s+\\Z\", \"\", string)\n",
    "\n",
    "def load_from_file(path, rule_template=False):\n",
    "    \"\"\"\n",
    "    - function: load and parsing file\n",
    "    :param path: file's location \n",
    "    :param rule_template: check rule file\n",
    "    \n",
    "    :return : parsed kb or rule template\n",
    "    \"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        text = f.readlines()\n",
    "        text = [x for x in text if not x.startswith(\"%\") and x.strip() != \"\"]\n",
    "        text = \"\".join(text)\n",
    "        rules = [x for x in re.split(\"\\.\\n|\\.\\Z\", text) if x != \"\" and\n",
    "                 x != \"\\n\" and not x.startswith(\"%\")]\n",
    "        kb = parse_rules(rules, rule_template=rule_template)\n",
    "        return kb\n",
    "    \n",
    "def parse_rules(rules, rule_template=False):\n",
    "    \"\"\"\n",
    "    - function: read file and parse rules\n",
    "    input : list of strings (such as 2 #1(X, Y) :- #2(X, Z),#3(Z, W),#4(W, Y))\n",
    "    output : list of lists (such as [('#1', 'X', 'Y'), ('#2', 'X', 'Z'), ('#3', 'Z', 'W'), ('#4', 'W', 'Y'), 2]) \n",
    "    \"\"\"\n",
    "    kb = []\n",
    "    for rule in rules:\n",
    "        num = rule[:rule.find('\\t')]\n",
    "        rule = re.findall(r'#\\d+\\(.*?\\)', rule)\n",
    "        listAtoms = [re.split('[(),]', item)[:-1] for item in rule ]\n",
    "        atoms = [(pred, sub, obj.strip()) for [pred, sub, obj] in listAtoms]\n",
    "        atoms.append(int(num))\n",
    "        kb.append(atoms)\n",
    "       \n",
    "    return kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('#1', 'X', 'Y'), ('#2', 'X', 'Z'), ('#3', 'Z', 'Y'), 2],\n",
       " [('#1', 'X', 'Y'), ('#2', 'X', 'Y'), 2]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules = load_from_file(f'./data/{data_name}.nlt', rule_template=True)\n",
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>rule_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'X': 'subj', 'Y': 'obj'}</td>\n",
       "      <td>{'X': 'subj', 'Z': 'obj'}</td>\n",
       "      <td>{'Z': 'subj', 'Y': 'obj'}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'X': 'subj', 'Y': 'obj'}</td>\n",
       "      <td>{'X': 'subj', 'Y': 'obj'}</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0                          1  \\\n",
       "0  {'X': 'subj', 'Y': 'obj'}  {'X': 'subj', 'Z': 'obj'}   \n",
       "1  {'X': 'subj', 'Y': 'obj'}  {'X': 'subj', 'Y': 'obj'}   \n",
       "\n",
       "                           2  rule_number  \n",
       "0  {'Z': 'subj', 'Y': 'obj'}            0  \n",
       "1                       None            1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule_structure = pd.DataFrame(list(map(lambda x : [{atom[1]: 'subj', atom[2]: 'obj'} for atom in x[:-1]], rules)))\n",
    "rule_structure['rule_number'] = [i for i in range(len(rules))]\n",
    "rule_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dictionary from KG & Query data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "KG_predicate_list = sorted(set(KG.pred.values).union(set(Query.pred.values)))\n",
    "\n",
    "rule_pred_list = []\n",
    "for i, rule in enumerate(rules):\n",
    "    # iterate rule components\n",
    "    for r in rule[:-1]:\n",
    "        #iterate augmnet number\n",
    "        for j in range(rule[-1]):\n",
    "            suffix = '_' + str(i) + '_' + str(j)\n",
    "            rule_pred_list.append(r[0]+suffix)\n",
    "            \n",
    "predicate_list = sorted(set(KG_predicate_list).union(set(rule_pred_list)))\n",
    "# print('predicates : ',predicate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2sym_dict = {}\n",
    "sym2id_dict = {}\n",
    "sym2id_dict['UNK'] = 0\n",
    "sym2id_dict['PAD'] = 1\n",
    "id2sym_dict[0] = 'UNK'\n",
    "id2sym_dict[1] = 'PAD'\n",
    "\n",
    "\n",
    "for i, p in enumerate(predicate_list):\n",
    "    sym2id_dict[p] = i+2\n",
    "    id2sym_dict[i+2] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 0,\n",
       " 'PAD': 1,\n",
       " '#1_0_0': 2,\n",
       " '#1_0_1': 3,\n",
       " '#1_1_0': 4,\n",
       " '#1_1_1': 5,\n",
       " '#2_0_0': 6,\n",
       " '#2_0_1': 7,\n",
       " '#2_1_0': 8,\n",
       " '#2_1_1': 9,\n",
       " '#3_0_0': 10,\n",
       " '#3_0_1': 11,\n",
       " 'bornIn': 12,\n",
       " 'hasFather': 13,\n",
       " 'locatedIn': 14,\n",
       " 'nationality': 15,\n",
       " 'placeOfBirth': 16}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sym2id_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions \n",
    "\n",
    "### unification\n",
    "- goal: query (e.g. nationality BART USA)\n",
    "- rule: rule template (e.g. #1(X,Y) :- #2(X,Z), #3(Z,Y))\n",
    "\n",
    "- 1. 주어진 rule template의 conclusion과 query를 unify\n",
    "    - unify된 트리플은 rule component substitution에 key를 rule component(e.g. #1(X,Y))로  \n",
    "        value를 unified triples(dataframe)으로 저장   \n",
    "    \n",
    "        #1(X, Y) :\n",
    "    \n",
    "            |     pred    | subj | obj |\n",
    "            |-------------|------|-----|\n",
    "            | nationality | BART | USA |\n",
    "    \n",
    "    - conclusion의 X,Y와 같은 variable에 대하여 unify된 트리플을 참조하여 variable substitution에  \n",
    "    X : [BART], Y: [USA] 와 같이 binding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- 2. 앞서 binding된 variable을 참조하여 각 rule body에 맞는 트리플을 unify\n",
    "    - #1(X,Y)를 통해 binding된 X에 대한 variable substitution을 참조하여 #2(X,Z)와 같은 body에 트리플을 unify하는 작업을 수행\n",
    "        - 위 경우에는 variable substitution을 참조하여 X가 subject인 트리플을 찾아 unify   \n",
    "    - unify된 트리플은 rule component substitution에 key를 rule component(e.g. #2(X,Y))로  \n",
    "        value를 unified triples(dataframe)으로 저장   \n",
    "           \n",
    "       #2(X, Z) :\n",
    "\n",
    "            |     pred     | subj | obj     |\n",
    "            |--------------|------|---------|\n",
    "            | placeOfBirth | BART | NEWYORK |\n",
    "            | hasFather    | BART | HOMMER  |    \n",
    "        \n",
    "    - 규칙 body의 X,Z와 같은 variable에 대하여 unify된 트리플을 참조하여 variable substitution에  \n",
    "    Z : [NEWYORK, HOMMER] 와 같이 binding\n",
    "    \n",
    "### proof path completion\n",
    "- rule component substitution : key가 rule compnent (e.g. #1(X,Y)) value가 각 rule component에 unify된 트리플(dataframe)인 dictionary \n",
    "- rule: rule template (e.g. #1(X,Y) :- #2(X,Z), #3(Z,Y))\n",
    "- 1. rule template을 분석하여 인접한 rule component간의 common variable 도출 \n",
    "- 2. common variable을 기준으로 unified triple을 join하여 proof path를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stackoverflow Check multi-value duplication in pandas\n",
    "def check_duplicate(proof_path):\n",
    "    \n",
    "    triple_size = 3\n",
    "    column_len = len(proof_path.columns)\n",
    "    num_unique_triples = int(column_len/triple_size) \n",
    "\n",
    "    proof_path['n_unique_triples'] = \\\n",
    "        proof_path.apply(lambda row: len(set([tuple(row[ i*triple_size : (i+1)*triple_size ]) \n",
    "                                                    for i in range(num_unique_triples)])), axis=1)\n",
    "\n",
    "    proof_path = proof_path[proof_path.n_unique_triples == num_unique_triples]\n",
    "    proof_path\n",
    "    \n",
    "    return proof_path\n",
    "\n",
    "#stackoverflow How to insert a dropped join key column from Dataframe join in order\n",
    "def merge_from_common_key(current_rComp, next_rComp):\n",
    "\n",
    "    key = current_rComp.columns.intersection(next_rComp.columns, sort=False)\n",
    "    new_key = key + '_'\n",
    "    d = dict(zip(key, new_key))\n",
    "\n",
    "    proof_path = pd.merge(current_rComp, next_rComp.rename(columns=d), how='inner', \n",
    "                          left_on=key.tolist(), right_on=new_key.tolist())\n",
    "\n",
    "    return proof_path\n",
    "\n",
    "def proof_path_completion(rComp_substitution, rule):\n",
    "    \n",
    "    for depth in range(len(rule)-2):    \n",
    "        if depth == 0:\n",
    "            #Load dataframes\n",
    "            current_rComp = rComp_substitution[rule[depth]] \n",
    "            next_rComp = rComp_substitution[rule[depth+1]]\n",
    "            #merge from common key\n",
    "            partial_proof_path = merge_from_common_key(current_rComp, next_rComp)\n",
    "\n",
    "        else:\n",
    "            #Load dataframes\n",
    "            current_rComp = partial_proof_path\n",
    "            next_rComp = rComp_substitution[rule[depth+1]]\n",
    "            #merge from common key\n",
    "            partial_proof_path = merge_from_common_key(current_rComp, next_rComp)\n",
    "\n",
    "    #remove duplicate  Adjacent triple\n",
    "    proof_path = partial_proof_path\n",
    "    proof_path = check_duplicate(proof_path)\n",
    "    \n",
    "    #stackoverflow How to get that result without for loop (python)\n",
    "    unified_rel_path = proof_path[[r[0] for r in rule[:-1]]].drop_duplicates()\\\n",
    "           .apply(lambda x: list(zip([r[0] for r in rule[:-1]], x)), axis=1).values.tolist()\n",
    "    \n",
    "    return unified_rel_path\n",
    "\n",
    "def unification(goal, rule, KG, KG_index, rule_num, depth = 0, \n",
    "                variable_substitution={}, rComp_substitution={}, rule_structure=None):\n",
    "    '''\n",
    "    - function: \n",
    "        1. Unify Variables and store information in substitution dictionary\n",
    "        2. Check Common Variable from Rules and Join each Triples\n",
    "        \n",
    "    :param goal: a query triple (e.g. [nationality BART USA])\n",
    "    :parma rule: a given rule template (e.g. [#1(X,Y) :- #2(Y,X), 2])\n",
    "    :param depth: an integer indicates rule depth\n",
    "    :param variable_substitution: a dictionary which has information of unified variables\n",
    "        - key: variable / value : unified entity (list)\n",
    "    :param rComp_substitution : a dictionary which has information of unified rule components\n",
    "        - key: rule compoenet / value : unified triples (dataframe)\n",
    "    :return: proof paths generated by Symbolic Unification \n",
    "    '''\n",
    "\n",
    "    if depth == 0 :\n",
    "        rComp_substitution[rule[depth]] = pd.DataFrame(goal, index=[rule[depth][0],rule[depth][1],rule[depth][2]]).transpose()\n",
    "        # subject variable binding\n",
    "        if rule[depth][1] not in variable_substitution.keys():\n",
    "            variable_substitution[rule[depth][1]] = [goal[1]]\n",
    "        # object variable binding\n",
    "        if rule[depth][2] not in variable_substitution.keys():\n",
    "            variable_substitution[rule[depth][2]] = [goal[2]]   \n",
    "        depth += 1\n",
    "        \n",
    "    if depth == len(rule)-1:\n",
    "\n",
    "        return proof_path_completion(rComp_substitution, rule)\n",
    "\n",
    "    else :\n",
    "        common_variable = []\n",
    "        current_body = rule_structure[rule_structure['rule_number'] == rule_num].iloc[0,depth]\n",
    "        current_variable = list(current_body.keys())\n",
    "        common_variable = [variable for variable in current_variable if variable in variable_substitution]\n",
    "        cVar_position = list(map(current_body.get, common_variable)) \n",
    "        unified_cVar= list(map(variable_substitution.get, common_variable))\n",
    "        \n",
    "        if len(common_variable) == 1:\n",
    "            cVar_index = set(itertools.chain.from_iterable(\n",
    "                             list(map(lambda x : KG_index.get(x).get(cVar_position[0]), unified_cVar[0]))))\n",
    "\n",
    "            sub_goal = KG.loc[cVar_index]\n",
    "            sub_goal.columns = [rule[depth][0], rule[depth][1], rule[depth][2]]\n",
    "            rComp_substitution[rule[depth]] = sub_goal\n",
    "\n",
    "            # subject variable binding\n",
    "            if rule[depth][1] not in variable_substitution.keys():\n",
    "                variable_substitution[rule[depth][1]] = list(set(sub_goal[rule[depth][1]].values))\n",
    "            # object variable binding\n",
    "            if rule[depth][2] not in variable_substitution.keys():\n",
    "                variable_substitution[rule[depth][2]] = list(set(sub_goal[rule[depth][2]].values))\n",
    "\n",
    "        else : \n",
    "            subj_cVar_index = set(itertools.chain.from_iterable(\n",
    "                                  list(map(lambda x : KG_index.get(x).get(cVar_position[0]), unified_cVar[0]))))\n",
    "            obj_cVar_index = set(itertools.chain.from_iterable(\n",
    "                                  list(map(lambda x : KG_index.get(x).get(cVar_position[1]), unified_cVar[1]))))\n",
    "            sub_goal = KG.loc[subj_cVar_index &obj_cVar_index]\n",
    "            sub_goal.columns = [rule[depth][0], rule[depth][1], rule[depth][2]]\n",
    "            rComp_substitution[rule[depth]] = sub_goal      \n",
    "        depth += 1\n",
    "       \n",
    "        return unification(goal, rule, KG, KG_index, rule_num, depth, \n",
    "                           variable_substitution, rComp_substitution,rule_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Symbolic Unification \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unify_dict(unified_rel_path):\n",
    "    # stakcoverflow Easy way to map values from list of list to dictionary\n",
    "    unify_dict = collections.defaultdict(set)\n",
    "    unified_rel_path = list(itertools.chain.from_iterable(unified_rel_path))\n",
    "    for key, value in unified_rel_path:\n",
    "        unify_dict[key].add(value)\n",
    "    return unify_dict\n",
    "\n",
    "def negative_sampling(unified_rel_path, unify_dict, KG):\n",
    "    \n",
    "    neg_unified_rel_path = copy.deepcopy(unified_rel_path)\n",
    "    for path_idx, path in enumerate(neg_unified_rel_path):\n",
    "        for group_idx, group in enumerate(path):\n",
    "            rule_pred = group[0]\n",
    "            pos_pred = unify_dict[rule_pred]\n",
    "            KG_pred = set(KG['pred'])\n",
    "            neg_pred = KG_pred - pos_pred\n",
    "\n",
    "            if len(neg_pred) == 0:\n",
    "                neg_unified_rel_path[path_idx][group_idx] = (rule_pred, 'UNK')\n",
    "            else:\n",
    "                neg_sym = random.choice(list(neg_pred))\n",
    "                neg_unified_rel_path[path_idx][group_idx] = (rule_pred, neg_sym)\n",
    "                \n",
    "    return neg_unified_rel_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete generating proof paths! : 7/7\n"
     ]
    }
   ],
   "source": [
    "def generate_batches(query, KG, rules):\n",
    "    number = 0\n",
    "    relation_path = []\n",
    "    rule_temp_path = []\n",
    "    max_path = 0\n",
    "    for row in Query.itertuples(index=False):\n",
    "        number += 1\n",
    "        if number%100 == 0:\n",
    "            print('generate proof paths : '+str(number)+'/'+str(len(Query)))\n",
    "        elif number == len(Query):\n",
    "            print('complete generating proof paths! : '+str(number)+'/'+str(len(Query)))\n",
    "        goal = list(row)\n",
    "        aug_rel_path_list = []\n",
    "        aug_rule_temp_path_list = []\n",
    "        for rule_num, rule in enumerate(rules):     \n",
    "            unified_rel_path = unification(goal, rule, KG, KG_index,\n",
    "                                                rule_num = rule_num, depth = 0, variable_substitution={}, \n",
    "                                                rComp_substitution={}, rule_structure = rule_structure)\n",
    "            if len(unified_rel_path) > 0 :\n",
    "                if max_path < len(unified_rel_path):\n",
    "                    max_path = len(unified_rel_path)\n",
    "                \n",
    "                aug_rel_path = [[list(map(lambda x : sym2id_dict[x[1]], path))]*rule[-1] \n",
    "                                 for path in unified_rel_path]\n",
    "                aug_rule_temp_path = [[list(map(lambda x : sym2id_dict[f'{x[0]}_{str(rule_num)}_{str(aug_num)}'], path)) \n",
    "                                      for aug_num in range(rule[-1])] for path in unified_rel_path]\n",
    "                \n",
    "                #negative sampling\n",
    "                unify_dict = create_unify_dict(unified_rel_path)\n",
    "                for i in range(neg_per_pos):\n",
    "                    neg_unified_rel_path = negative_sampling(unified_rel_path, unify_dict, KG)\n",
    "                    neg_aug_rel_path = [[list(map(lambda x : sym2id_dict[x[1]], path))]*rule[-1] \n",
    "                                      for path in neg_unified_rel_path]\n",
    "                    aug_rel_path += neg_aug_rel_path\n",
    "                aug_rule_temp_path += aug_rule_temp_path * neg_per_pos\n",
    "\n",
    "                aug_rel_path_list.append(aug_rel_path)\n",
    "                aug_rule_temp_path_list.append(aug_rule_temp_path)\n",
    "            else :\n",
    "                aug_rel_path_list.append([])\n",
    "                aug_rule_temp_path_list.append([])\n",
    "        relation_path.append(tuple(aug_rel_path_list))\n",
    "        rule_temp_path.append(tuple(aug_rule_temp_path_list))\n",
    "\n",
    "    return relation_path, rule_temp_path, max_path\n",
    "\n",
    "relation_path, rule_temp_path, max_path = generate_batches(Query, KG, rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for debuging\n",
    "# relation_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for debuging\n",
    "# rule_temp_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data filtering\n",
    "    - proof path가 없는 데이터 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(iter_object):\n",
    "    for element in iter_object:\n",
    "        if isinstance(element, Iterable):\n",
    "            yield from flatten(element)\n",
    "        else:\n",
    "            yield element\n",
    "            \n",
    "def data_filter(path_to_query):\n",
    "    path_existence = True\n",
    "    if len(list(flatten(path_to_query))) == 0:\n",
    "        path_existence = False\n",
    "    return path_existence\n",
    "\n",
    "relation_path = list(filter(data_filter, relation_path))\n",
    "rule_temp_path = list(filter(data_filter, rule_temp_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(relation_path, rule_temp_path, rules, max_path):\n",
    "    \n",
    "    comp_each_template = []\n",
    "    for rule in rules:\n",
    "        comp_each_template.append(len(rule)-1)\n",
    "\n",
    "    single_temp_size = 1 + (1 * neg_per_pos)\n",
    "    for query_idx, (rel_path_to_query, rule_temp_path_to_query) in \\\n",
    "        enumerate(zip(relation_path, rule_temp_path)):\n",
    "        rel_path_to_query = list(rel_path_to_query)\n",
    "        rule_temp_path_to_query = list(rule_temp_path_to_query)\n",
    "        \n",
    "        for template_idx, (rel_path_to_template, rule_temp_path_to_template) in \\\n",
    "            enumerate(zip(rel_path_to_query, rule_temp_path_to_query)):\n",
    "            if len(rel_path_to_template) == 0:\n",
    "                padding = np.ones((max_path*single_temp_size, rules[template_idx][-1], comp_each_template[template_idx]), \n",
    "                                  dtype=int).tolist()\n",
    "                rel_path_to_query[template_idx] = padding\n",
    "                rule_temp_path_to_query[template_idx] = padding\n",
    "\n",
    "            elif len(rel_path_to_template) != 0:\n",
    "                pad_rel_path_to_template = []\n",
    "                pad_rule_temp_path_to_template = []\n",
    "                num_pos_path = int(len(rel_path_to_template)/single_temp_size)\n",
    "                for i in range(0, len(rel_path_to_template), num_pos_path):\n",
    "                    pad_rel_path_to_template += rel_path_to_template[i:i+num_pos_path]\n",
    "                    pad_rule_temp_path_to_template += rule_temp_path_to_template[i:i+num_pos_path]\n",
    "                    padding = np.ones((max_path-num_pos_path, rules[template_idx][-1],comp_each_template[template_idx]),\n",
    "                                      dtype=int).tolist()\n",
    "                    pad_rel_path_to_template += padding\n",
    "                    pad_rule_temp_path_to_template += padding\n",
    "                rel_path_to_query[template_idx] = pad_rel_path_to_template\n",
    "                rule_temp_path_to_query[template_idx] = pad_rule_temp_path_to_template\n",
    "        relation_path[query_idx] = tuple(rel_path_to_query)\n",
    "        rule_temp_path[query_idx] = tuple(rule_temp_path_to_query)\n",
    "        \n",
    "    return relation_path, rule_temp_path\n",
    "\n",
    "relation_path, rule_temp_path = padding(relation_path, rule_temp_path, rules, max_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for debuging\n",
    "#rule_temp_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for debuging\n",
    "#relation_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Relation Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_sim(embed_aug_rule_temp_path, embed_aug_rel_path):\n",
    "    eps = 1e-6\n",
    "    #stackoverflow To calculate euclidean distance between vectors in a torch tensor with multiple dimensions\n",
    "    dist = torch.sqrt((embed_aug_rel_path - embed_aug_rule_temp_path).pow(2).sum(3)+eps)\n",
    "    sim = torch.exp(-dist)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTP(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size, batch_size, num_templates):\n",
    "        super(NTP, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding_matrix = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.loss = torch.nn.BCELoss()\n",
    "        self.batch_size = batch_size\n",
    "        self.template_size = num_templates\n",
    "    \n",
    "    def calculate_sim_avg(self, aug_rule_temp_path, aug_rel_path):\n",
    "        sims_list = []\n",
    "\n",
    "        for i in range(self.template_size):\n",
    "            if len(aug_rule_temp_path[i]) == 0:\n",
    "                continue\n",
    "            if len(aug_rel_path[i]) == 0:\n",
    "                continue\n",
    "            lookup_tensor_aug_rule_temp_path = torch.tensor(aug_rule_temp_path[i], dtype=torch.long)\n",
    "            lookup_tensor_aug_rel_path = torch.tensor(aug_rel_path[i], dtype=torch.long)\n",
    "            embed_aug_rule_temp_path = self.embedding_matrix(lookup_tensor_aug_rule_temp_path)\n",
    "            embed_aug_rel_path = self.embedding_matrix(lookup_tensor_aug_rel_path)\n",
    "            sims=l2_sim(embed_aug_rule_temp_path, embed_aug_rel_path)\n",
    "            avg_sims = torch.mean(sims, 2)\n",
    "            sims_list.append(avg_sims)\n",
    "            \n",
    "        avg_sims_ = torch.cat(sims_list, dim=1)\n",
    "\n",
    "        return avg_sims_\n",
    "        \n",
    "        \n",
    "    def forward(self, aug_rule_temp_path, aug_rel_path):\n",
    "        avg_sims = self.calculate_sim_avg(aug_rule_temp_path, aug_rel_path)\n",
    "        x = torch.chunk(avg_sims, self.batch_size, dim=0)\n",
    "        x = list(x)\n",
    "        for i, t in enumerate(x):\n",
    "            x[i] = torch.cat(torch.chunk(t, chunks =self.template_size ,dim=1), dim=0)#template\n",
    "        sims = torch.cat(x, dim=0)\n",
    "        max_sims = torch.max(sims, axis=1)[0]\n",
    "        max_sims = max_sims.reshape(self.batch_size, -1)\n",
    "        min_sims = torch.min(max_sims, axis=1)[0]\n",
    "        \n",
    "        return min_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_templates = len(rules)\n",
    "vocab_size = len(sym2id_dict)\n",
    "embedding_size = 100\n",
    "ntp = NTP(vocab_size, embedding_size, batch_size, num_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = []\n",
    "for i in range(pos_per_batch):\n",
    "    answer += [1]\n",
    "    for j in range(neg_per_pos):\n",
    "        answer += [0]\n",
    "answer = torch.tensor(answer, dtype=torch.float32)\n",
    "answer = answer\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10       Loss :  0.6187537908554077\n",
      "Epoch:  20       Loss :  0.6109741926193237\n",
      "Epoch:  30       Loss :  0.577885627746582\n",
      "Epoch:  40       Loss :  0.8521082997322083\n",
      "Epoch:  50       Loss :  0.48950350284576416\n",
      "Epoch:  60       Loss :  0.4862099885940552\n",
      "Epoch:  70       Loss :  0.453716903924942\n",
      "Epoch:  80       Loss :  0.5597609877586365\n",
      "Epoch:  90       Loss :  0.5289940237998962\n",
      "Epoch:  100       Loss :  0.4599451720714569\n",
      "\n",
      "training time :  0:00:03.649883\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "report_interver_epoch = 10\n",
    "optimizer = torch.optim.Adam(ntp.parameters(), lr = 0.08, weight_decay = 0.00001)\n",
    "data_size = len(relation_path)\n",
    "ntp.train()\n",
    "time1 = datetime.now()\n",
    "for epoch in range(1, epochs+1):\n",
    "    for i in range(0, data_size, pos_per_batch):\n",
    "        optimizer.zero_grad()\n",
    "        aug_rel_path = []\n",
    "        aug_rule_temp_path = []\n",
    "        r1 = rule_temp_path[i:i+pos_per_batch]\n",
    "        r2 = relation_path[i:i+pos_per_batch]\n",
    "        if len(r1)<pos_per_batch:\n",
    "            continue\n",
    "        #stackoverflow An easy way to create a torch tensor from multiple elements of tuple through concatenate\n",
    "        aug_rule_temp_path = [torch.Tensor([ atom for element in x for atom in element ]) for x in zip(*r1)]\n",
    "        aug_rel_path = [torch.Tensor([ atom for element in x for atom in element ]) for x in zip(*r2)]\n",
    "        y_hat = ntp.forward(aug_rule_temp_path, aug_rel_path)\n",
    "        answer = answer\n",
    "        loss = ntp.loss(y_hat, answer)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch%report_interver_epoch == 0:\n",
    "        print('Epoch: ',epoch, '      Loss : ', loss.item())\n",
    "\n",
    "time2 = datetime.now()\n",
    "print('\\ntraining time : ', time2-time1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write rule file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representation_match(x, emb):\n",
    "    dist = torch.torch.nn.functional.pairwise_distance(x, emb)\n",
    "    sim = torch.exp(-dist)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.6943e-10, -1.1226e-10, -1.0804e-09,  ...,  9.3667e-11,\n",
      "         -9.7855e-11,  6.3358e-10],\n",
      "        [ 9.4268e-10,  2.4035e-10,  1.9959e-09,  ...,  2.1840e-09,\n",
      "         -5.1814e-11,  4.1839e-10],\n",
      "        [-1.7726e-01,  1.4352e-01,  3.1908e-01,  ...,  3.9006e-01,\n",
      "         -3.7725e-01,  1.7340e-01],\n",
      "        ...,\n",
      "        [-6.9433e-03,  1.7421e-01,  3.9733e-02,  ...,  5.4037e-01,\n",
      "         -2.0568e-01,  8.2744e-01],\n",
      "        [-1.9272e-01, -4.5514e-03,  3.3609e-01,  ...,  2.5940e-01,\n",
      "         -3.5859e-01,  1.1728e-01],\n",
      "        [-8.4288e-01, -1.6064e-01, -1.8080e-01,  ..., -2.9404e-01,\n",
      "         -6.1306e-01,  3.3202e-01]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#get trained embedding matrix\n",
    "for i in enumerate(ntp.parameters()):\n",
    "    print(i[1])\n",
    "    embeddings = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(('p0', 'X', 'Y'),\n",
       "  ('p1', 'X', 'Z'),\n",
       "  ('p2', 'Z', 'Y')): [[[2, 'X', 'Y'],\n",
       "   [6, 'X', 'Z'],\n",
       "   [10, 'Z', 'Y']], [[3, 'X', 'Y'], [7, 'X', 'Z'], [11, 'Z', 'Y']]],\n",
       " (('p0', 'X', 'Y'), ('p1', 'X', 'Y')): [[[4, 'X', 'Y'], [8, 'X', 'Y']],\n",
       "  [[5, 'X', 'Y'], [9, 'X', 'Y']]]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get parameterized rule template\n",
    "rule_templates = {}\n",
    "ids_rule_templates = {}\n",
    "for rule_number, template in enumerate(rules):\n",
    "    result_template_key = []\n",
    "    ids_result_template_value = []\n",
    "    ids_result_template_values = []\n",
    "    for i in range(len(template)-1):\n",
    "        rule_element=('p'+ str(int(template[i][0][1])-1), template[i][1], template[i][2])       \n",
    "        result_template_key.append(rule_element)\n",
    "        rule_element = ()\n",
    "\n",
    "    for aug in range(template[-1]):\n",
    "        for j in range(len(template)-1):\n",
    "            ids_result_template_value.append([sym2id_dict[template[j][0]+'_'+str(rule_number)+'_'+\n",
    "                                                           str(aug)], template[j][1], template[j][2]])\n",
    "        ids_result_template_values.append(ids_result_template_value)\n",
    "        ids_result_template_value = []\n",
    "    ids_rule_templates[tuple(result_template_key)] = ids_result_template_values\n",
    "\n",
    "ids_rule_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[((('p0', 'X', 'Y'), ('p1', 'X', 'Z'), ('p2', 'Z', 'Y')),\n",
       "   0.00034720447729341686,\n",
       "   ['nationality(X,Y)', 'hasFather(X,Z)', 'nationality(Z,Y)'])],\n",
       " [((('p0', 'X', 'Y'), ('p1', 'X', 'Z'), ('p2', 'Z', 'Y')),\n",
       "   0.39551132917404175,\n",
       "   ['nationality(X,Y)', 'placeOfBirth(X,Z)', 'locatedIn(Z,Y)'])],\n",
       " [((('p0', 'X', 'Y'), ('p1', 'X', 'Y')),\n",
       "   0.035376399755477905,\n",
       "   ['bornIn(X,Y)', 'nationality(X,Y)'])],\n",
       " [((('p0', 'X', 'Y'), ('p1', 'X', 'Y')),\n",
       "   0.10968772321939468,\n",
       "   ['nationality(X,Y)', 'bornIn(X,Y)'])]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get rule instance & write rule file\n",
    "masking_index = []\n",
    "for key, value in ids_rule_templates.items():\n",
    "    for rule in value:\n",
    "        for element in rule:\n",
    "            masking_index.append(element[0])\n",
    "        \n",
    "masking_index\n",
    "\n",
    "total_reuslt = []\n",
    "with open(data_name+'_rule.nl', 'w') as f:\n",
    "    for key, value in ids_rule_templates.items():\n",
    "        f.write(str(key)+'\\n')\n",
    "        for rule in value:\n",
    "            result = []\n",
    "            confidence_score = []\n",
    "            rule_result = []\n",
    "            for element in rule:\n",
    "                masking_index = masking_index+[element[0]]+[0, 1]\n",
    "                x = ntp.embedding_matrix(torch.tensor([element[0]]))\n",
    "                match = representation_match(x, embeddings)\n",
    "                match[masking_index] = 0\n",
    "                top_k = torch.topk(match, 1)\n",
    "                rule_result.append(id2sym_dict[top_k.indices.item()]+'('+element[1]+','+element[2]+')')\n",
    "                confidence_score.append(match[top_k.indices])\n",
    "            f.write(str(min(confidence_score).item())+'\\t')\n",
    "            head = rule_result[0]\n",
    "            body = rule_result[1:]\n",
    "            f.write(head + ' :- ' +\", \".join(body)+'\\n')  \n",
    "            result.append((key, min(confidence_score).item(), rule_result))\n",
    "            total_reuslt.append(result)\n",
    "        f.write('\\n')\n",
    "total_reuslt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_name+'_rule_batch'+str(batch_size)+'_epoch'+str(epochs)+\n",
    "          '_sorted_'+str(time2)[11:13]+str(time2)[14:16]+'.nl', 'w') as file:\n",
    "    with open(data_name+'_rule.nl', 'r') as f:\n",
    "        augment = rules[0][-1]\n",
    "        scores = []\n",
    "        total_scores = []\n",
    "        rule = []\n",
    "        total_rules = []\n",
    "        count = 0\n",
    "        for line in f:\n",
    "\n",
    "            if '.' not in line.split('\\t')[0]:\n",
    "                file.write(line.split('\\t')[0])\n",
    "            if '.' in line.split('\\t')[0]:\n",
    "                count+=1\n",
    "\n",
    "                scores.append(round(float(line.split('\\t')[0]), 8))\n",
    "                rule.append(line.split('\\t')[-1])\n",
    "                if count % augment == 0:\n",
    "                    count = 0\n",
    "                    total_scores.append(scores)\n",
    "                    total_rules.append(rule)\n",
    "                    s = torch.sort(torch.tensor(scores), descending=True).values\n",
    "                    r = torch.sort(torch.tensor(scores), descending=True).indices\n",
    "\n",
    "                    for i in range(augment):\n",
    "                        file.write(str(round(s[i].item(), 8))+'\\t')\n",
    "                        file.write(rule[r[i].item()])\n",
    "                    scores = []\n",
    "                    rule = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
